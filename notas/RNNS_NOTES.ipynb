{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1804178-0abb-48c0-8da3-ee5e958fa5f5",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes(RNN's)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a24b68-0569-4545-affe-3f2ed2dc052b",
   "metadata": {},
   "source": [
    "## *¿Alguna vez han pensado en cómo las máquinas entienden el contexto en el tiempo?*  \n",
    "## *¿Cómo sabe un teclado predictivo cuál es la próxima palabra que quiero escribir o cómo una app de traducción interpreta correctamente una frase completa?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bab584-c27d-4e6d-a08d-9e3413197378",
   "metadata": {},
   "source": [
    "![](Images/deeplearning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34940a0-b504-43d2-9f22-c225637458b7",
   "metadata": {},
   "source": [
    "## Las Redes Neuronales Recurrentes resuelven lo que las redes neuronales 'convencionales' no pueden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d236f51-590a-480e-981b-164a5203d11d",
   "metadata": {},
   "source": [
    "Las **Redes Neuronales Recurrentes (RNN)** son una **evolución** clave dentro del campo del aprendizaje profundo, diseñadas específicamente para superar una de las limitaciones más importantes de las redes neuronales convencionales: la incapacidad de **manejar datos secuenciales** o **dependencias temporales**. Mientras que una red neuronal tradicional procesa las entradas de manera aislada, las **RNN tienen una memoria interna** que les permite recordar información relevante a lo largo de secuencias de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1aae8-a06c-4b2c-af29-8b33eadd5b40",
   "metadata": {},
   "source": [
    "### Limitaciones de las Redes Neuronales y Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc28528-652a-4e74-95c0-b68af12caa72",
   "metadata": {},
   "source": [
    "**La importancia de las RNN radica en su capacidad para resolver problemas donde el orden y el contexto importan**. \n",
    "\n",
    "Las **Redes Neuronales** o las **Redes Convolucionales** permiten **procesar un solo dato a la vez** (como un sonido o una imagen), pero si tenemos una secuencia de sonidos (una conversación) o de imágenes (un video), este tipo de arquitecturas no estarán en capacidad de procesar este tipo de secuencias.\n",
    "\n",
    "Las Redes Neuronales Recurrentes resuelven este inconveniente, pues son capaces de procesar diferentes tipos de secuencias (como videos, conversaciones, texto) y, a diferencia de las redes neuronales o convolucionales, no se utilizan para clasificar un dato en particular sino que también están en capacidad incluso de **generar nuevas secuencias**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52808015-dbfb-4151-9a57-a58a2a77d434",
   "metadata": {},
   "source": [
    "### Ejemplo sencillo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e089f4d-e2e5-4116-ab34-40574c30c9cc",
   "metadata": {},
   "source": [
    "1. Predicción de texto en un teclado o autocompletado\n",
    "\n",
    "Las redes neuronales convencionales no tienen una memoria interna que les permita recordar las palabras previas que has escrito. Imagina que estás escribiendo una frase como:\n",
    "\n",
    "    \"Voy al parque con mi per...\"\n",
    "\n",
    "Para predecir correctamente que la siguiente palabra puede ser \"perro\", es necesario tener en cuenta las palabras anteriores (\"parque\" y \"mi\"). Una red neuronal convencional solo procesaría el input actual (\"per...\"), sin considerar el contexto, y podría hacer una predicción inexacta. En cambio, una RNN puede recordar el contexto anterior y usarlo para hacer predicciones más precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba46d38-109d-4876-a8b6-5b2f30ff6813",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](Images/mascota1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48bffa-33e6-445e-bf43-7438052ce12a",
   "metadata": {},
   "source": [
    "## Breve historia de las RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb126ec-e43b-43ec-9691-1f6a7eae9f7e",
   "metadata": {},
   "source": [
    "#### 2. Primeros desarrollos\n",
    "En los años 80, los investigadores querían extender las redes neuronales para que tuvieran **memoria**.  \n",
    "Una de las primeras ideas vino de **John Hopfield**, quien en 1982 desarrolló las **redes de Hopfield**, un precursor que ya tenía capacidad de **recordar patrones**.\n",
    "\n",
    "Este fue el inicio de la idea clave detrás de las RNN: que las redes pudieran \"recordar\" información de las entradas previas para usarla en predicciones futuras.\n",
    "![HOPFIELD](Images/Hopfield_John-Former-Faculty.3062f464.fill-1600x810-c100.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687a8b7-b196-466f-81f8-7484e8eb2e2f",
   "metadata": {},
   "source": [
    "#### 3. Retropropagación y RNN básicas\n",
    "En 1986, **David Rumelhart**, **Geoffrey Hinton**, y **Ronald J. Williams** sentaron las bases de las redes neuronales modernas con su algoritmo de **backpropagation** o retropropagación del error. Esto hizo posible entrenar redes de forma eficiente, ajustando los pesos de las conexiones.\n",
    "\n",
    "\n",
    "![](Images/geofreyhinton.jpeg)\n",
    "\n",
    "\n",
    "En 1989, Williams, junto a **David Zipser**, introdujo un avance clave para las RNN: el algoritmo de **Backpropagation Through Time (BPTT)**.  \n",
    "**Este algoritmo permitió aplicar retropropagación a las redes recurrentes para entrenarlas a través del tiempo**, es decir, aprendiendo de secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5be34-0ab9-4bb8-8392-d252a5cddfb7",
   "metadata": {},
   "source": [
    "#### 4. Los desafíos: El desvanecimiento del gradiente\n",
    "Sin embargo, las RNN enfrentaron un problema serio: ¿cómo hacer que las redes recuerden información en secuencias muy largas?\n",
    "\n",
    "El algoritmo de retropropagación enfrentaba el **desvanecimiento del gradiente**, lo que hacía que fuera difícil entrenar la red cuando las secuencias de datos eran largas.  \n",
    "Los gradientes se volvían tan pequeños que las actualizaciones en los pesos eran casi insignificantes.\n",
    "\n",
    "Por un tiempo, parecía que las RNN tenían una limitación importante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89af357-efe2-413c-a83c-340e7035e51f",
   "metadata": {},
   "source": [
    "#### 5. La solución: Long Short-Term Memory (LSTM)\n",
    "En 1997, **Sepp Hochreiter** y **Jürgen Schmidhuber** resolvieron este problema con las **LSTM** o **Long Short-Term Memory**.\n",
    "\n",
    "Las LSTM introdujeron una serie de \"puertas\" en su arquitectura que regulan el flujo de información, permitiendo a las redes olvidar o recordar lo que es relevante en el momento adecuado.  \n",
    "Esto hizo que las redes pudieran aprender dependencias a largo plazo con mayor efectividad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9241178-1feb-4217-bd60-9298a6b8e7e8",
   "metadata": {},
   "source": [
    "### 6. Impulsores recientes y auge de las RNN\n",
    "En la última década, investigadores como **Yoshua Bengio**, **Geoffrey Hinton**, y **Yann LeCun** ayudaron a consolidar las RNN, en especial con arquitecturas como las LSTM y las GRU.\n",
    "\n",
    "Estos avances han permitido que las RNN se utilicen en tareas como:\n",
    "- Procesamiento del lenguaje natural (chatbots, traducción automática).\n",
    "- Reconocimiento de voz (asistentes virtuales).\n",
    "- Predicción de series temporales (predicción de mercados financieros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8b624-d180-4c6b-91ac-0f22dffb5447",
   "metadata": {},
   "source": [
    "## Las Redes Neuronales Recurrentes vs. otras arquitecturas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6abffd-1188-4950-aef5-e919e03fd528",
   "metadata": {},
   "source": [
    "La **diferencia*** entre una **RNN** y otras arquitecturas como las **Redes neuronales** o **Convolucionales**, radica en el tipo de datos que pueden analizar. Las Redes Recurrentes están en capacidad de **analizar secuencias de datos**, las otras dos arquitecturas no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb66571-1748-450a-8981-354052b210c6",
   "metadata": {},
   "source": [
    "Si a una Red Neuronal o Convolucional se le presenta, por ejemplo, una **imagen** o una **palabra**, con el entrenamiento adecuado estas arquitecturas lograrán clasificar un sinnúmero de datos logrando a la vez una alta precisión.\n",
    "\n",
    "![](Images/CNN.png)\n",
    "![](Images/CNN-WORD.png)\n",
    "\n",
    "¿Pero qué pasa si en lugar de una única imagen o palabra, se introduce a la red una **secuencia de imágenes** (un video) o una **secuencia de palabras** (una conversación)?\n",
    "\n",
    "En este caso, ninguna de las redes será capaz de procesar los datos.\n",
    "\n",
    "En primer lugar **estas arquitecturas están diseñadas para que los datos de entrada y de salida siempre tengan el mismo tamaño**. Sin embargo, un video o una conversación se caracterizan por ser un tipo de dato con un tamaño variable.\n",
    "\n",
    "![](Images/videoframes.png)\n",
    "\n",
    "**En segundo lugar, en un video o en una conversación los datos están correlacionados**. \n",
    "\n",
    "\n",
    "Las Redes Neuronales Recurrentes no presentan este inconveniente que las dos anteriores si, y por tanto son capaces de analizar secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fb372-e891-4f0e-a23f-f016698809b4",
   "metadata": {},
   "source": [
    "## Conceptos importantes en RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54f21f-1d01-41dd-84d3-5cdafc46f7c4",
   "metadata": {},
   "source": [
    "#### Secuencias...\n",
    "**Una secuencia es una serie de datos (imágenes, palabras, notas musicales, sonidos) que siguen un orden específico y tienen únicamente significado cuando se analizan en conjunto y no de manera individual**.\n",
    "\n",
    "Por ejemplo, la palabra “secuencia” está conformada por diferentes caracteres (“s”-“e”-“c”-“u”-“e”-“n”-“c”-“i”-“a”). Dichos caracteres, analizados de forma individual o en un orden diferente (por ejemplo, “n”-“s”-“e”-“i”-“c”-“c”-“u”-“a”), carecen de significado.\n",
    "\n",
    "*IMPORTANTE:*\n",
    "1. No tiene un tamaño predefinido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579740b-7f3f-4859-a3d3-6893d0e510f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Correlación...\n",
    "\n",
    "**La correlación en el contexto de las redes neuronales recurrentes (RNN) se refiere a la relación que existe entre las entradas y salidas a lo largo del tiempo. En las RNN, los datos de entrada no son independientes entre sí, sino que tienen una estructura temporal, lo que significa que el estado actual de la red depende de las entradas anteriores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9d96b-2b1e-4712-9c05-5adb5b8befeb",
   "metadata": {},
   "source": [
    "## ¿Cómo funcionan las RNN's?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcd66d-518b-45ac-bf81-f721931822fb",
   "metadata": {},
   "source": [
    "Para lograr procesar una secuencia, las Redes Neuronales Recurrentes usan el concepto de **recurrencia**: para generar la salida, que en adelante llamaremos **activación**, la red **usa** no solo la entrada actual sino **la activación generada en la iteración previa**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d8a1b-0f3b-491f-9f34-7fba450194c1",
   "metadata": {},
   "source": [
    "![](Images/funcionamiento.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23eea3-f531-4585-8aa2-0a1781d98412",
   "metadata": {},
   "source": [
    "## Tipos de Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb482c5c-3e0d-46f8-a7fc-345dbfdaca58",
   "metadata": {},
   "source": [
    "### One to many\n",
    "La entrada es un único dato y la salida es una secuencia.\n",
    "\n",
    "\n",
    "![](Images/onetomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d8fd2-9517-4285-b927-4d5468d1ea22",
   "metadata": {},
   "source": [
    "### Many to one\n",
    "La entrada es una secuencia y la salida es por ejemplo una categoría.\n",
    "\n",
    "![](Images/manytoone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbc9c8-70b9-4c49-a4cc-b261d8f5f366",
   "metadata": {},
   "source": [
    "### Many to many\n",
    "\n",
    "La arquitectura “many to many” en donde tanto a la entrada como a la salida se tienen secuencias.\n",
    "\n",
    "En esta misma arquitectura “many to many” podemos encontrar los conversores de **voz-a-texto** o **texto a voz**, que son Redes Neuronales Recurrentes cuya entrada y salida es también una secuencia\n",
    "![](Images/manytomany.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c298b-973f-42d3-a402-e503d2ff46f2",
   "metadata": {},
   "source": [
    "## Aplicaciones de las RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae1e0c-5da8-4cb8-a636-4d632df1588f",
   "metadata": {},
   "source": [
    "## Image captioning\n",
    "\n",
    "Una de las primeras aplicaciones es el image captioning, en donde se usan dos modelos en conjunto: una Red Convolucional seguida por una Red Neuronal Recurrente. El resultado: el modelo entrenado es capaz de generar automáticamente texto que describe el contenido de la imagen.\n",
    "\n",
    "![](Images/Imagecaptioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c1f20-d085-4a3c-a197-f7bc483768a6",
   "metadata": {},
   "source": [
    "## Reconocimiento de escritura\n",
    "\n",
    "\n",
    "Recientemente Google desarrolló un módulo, ya disponible en los dispositivos Android, que permite el reconocimiento de escritura.\n",
    "\n",
    "\n",
    "![](Images/reconocimientoautomaticoescritura.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21db77-ede1-4c3b-886d-5b7e2eba79cd",
   "metadata": {},
   "source": [
    "## Más aplicaciones...\n",
    "\n",
    "* Comprensión del lenguaje.\n",
    "\n",
    "* Análisis de sentimientos\n",
    "\n",
    "* Generación de música\n",
    "\n",
    "* Detección de modificaciones en el ADN\n",
    "\n",
    "![](Images/aplicacionesrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d5268-5239-4613-b038-af0f46789813",
   "metadata": {},
   "source": [
    "## Estructura básica de una RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee28222-b985-4d4a-844d-5713bc127297",
   "metadata": {},
   "source": [
    "Recordemos que las **Redes Neuronales y Convolucionales sufren de amnesia**: para generar una salida sólo consideran la entrada actual, no entradas pasadas o futuras.\n",
    "\n",
    "Esta es la principal limitación de este tipo de redes, pues cuando hablamos de secuencias (como por ejemplo un texto, una conversación o un video) lo que nos interesa precisamente es que la red sea capaz de analizar el comportamiento de los datos en instantes previos (y posteriores) de tiempo.\n",
    "![](Images/diplosaurio1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a31531-bcef-4072-8b96-218c829e8a33",
   "metadata": {},
   "source": [
    "## Estructura de una red neuronal recurrente  \n",
    "\n",
    " **Instante de tiempo**: Para una secuencia, el instante de tiempo es simplemente un número entero que define la posición de cada elemento dentro de la secuencia.\n",
    "\n",
    " \n",
    "![](Images/diplosaurio2.png)\n",
    "\n",
    "\n",
    "\n",
    "Nos referiremos a $x_t$ como la entrada  a la red recurrente en el instante de tiempo $t$, y a $y_t$ como la salida en el instante de tiempo $t$.\n",
    "\n",
    "![](Images/diplosaurio3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974288b3-6930-42d2-980a-25dc7ed3a91f",
   "metadata": {},
   "source": [
    "**¿Cómo logra la Red Recurrente predecir correctamente el siguiente caracter en la secuencia?** \n",
    "\n",
    "\n",
    "Se observa que en cada instante de tiempo la red tiene realmente dos entradas y dos salidas.\n",
    "\n",
    "Las entradas son el dato actual $x_t$ y la activación anterior $a_{t-1}$, mientras que las salidas son la predicción actual $y_t$ y la activación actual $a_{t}$.\n",
    "Esta activación también recibe el nombre de **hidden state** o estado oculto.\n",
    "\n",
    "Estas **activaciones** las que corresponden precisamente a la **memoria de la red**, pues permiten **preservar y compartir la información entre un instante de tiempo y otro**.\n",
    "\n",
    "![](Images/entradasysalidasredrecurrente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6c9a8-9ab2-48c7-b2ad-f8257f769817",
   "metadata": {},
   "source": [
    "Para calcular la **salida y la activación de la Red Recurrente**, a partir de sus dos entradas, se usa la misma lógica de una Neurona Artificial convencional, es decir una neurona 'convencional' tiene una entrada $x$ y genera una salida $y$, y que la salida es el resultado de aplicar dos operaciones al dato de entrada: una transformación y una función de activación no-lineal.\n",
    "\n",
    "En el caso de las Redes Recurrentes, **la activación se calcula de manera similar**, y es el resultado primero de transformar los datos de entrada (es decir la activación anterior y la entrada actual) y luego llevarlos a una función de activación no-lineal.\n",
    "\n",
    "Los valores de los coeficientes W y b se calculan  con el mismo procedimiento usado en las Redes Neuronales, es decir con el entrenamiento.\n",
    "similarmente los coeficientes requeridos para el cálculo de la activación se obtienen mediante del entrenamiento de la Red Recurrente.\n",
    "\n",
    "![](Images/calculo-activacion-red-recurrente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169ce69-7038-457b-8ed8-e4843aa8f07f",
   "metadata": {},
   "source": [
    "De igual forma, para obtener la salida, se usa la activación del instante previo y se realizan las mismas operaciones (transformación y función de activación).\n",
    "\n",
    "![](Images/calculo-salida-red-recurrente2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623696b0-b037-4b91-97b7-55bc211a3cd2",
   "metadata": {},
   "source": [
    "En este esquema se muestra el comportamiento de la red para tres instantes de tiempo diferentes, sin embargo, es la misma red.\n",
    "\n",
    "![](Images/representacion-extendida-red-recurrente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92adca-da2b-4ffa-b220-6380db44a3c1",
   "metadata": {},
   "source": [
    "## En resumen ...\n",
    "La Red Recurrente tiene **dos entradas**: el **dato actual** y la **predicción anterior**. Al **combinar** estos dos elementos (usando una transformación y una función lineal, similares a las usadas en la Neurona Artificial), es posible **generar la salida de la red** así como **preservar la información obtenida en instantes de tiempo anteriores**, lo que equivale precisamente a la memoria de la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f79699-c037-4b33-98a5-6f235decd5dc",
   "metadata": {},
   "source": [
    "## LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4f3a3-a756-45fb-88e0-7837282606f7",
   "metadata": {},
   "source": [
    "### Limitaciones de las RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51776fd3-a064-4eed-a153-b0a3f45f8e0e",
   "metadata": {},
   "source": [
    "Veamos el porque se dice que las RNN's tienen **memoria de corto plazo**  para entender esto mejor usemos un ejemplo en el cual se  mostrara el **efecto que tienen los estados ocultos en la salida** para ello supongamos que se usa  la función de activación tangente hiperbólica para generar los estados ocultos y la función softmax para generar las predicciones y además omitiremos  el efecto que la entrada y el parámetro “b” tienen en el cálculo exacto de cada uno de estos estados.\n",
    "\n",
    "Supongamos que queremos calcular $y_3$ y  tomamos el estado oculto 3, lo multiplicamos por el parámetro de la red y lo llevamos a la función de activación softmax.\n",
    "\n",
    "Este estado oculto 3 se calcula a partir del estado oculto 2, que a su vez depende del estado 1 y del estado cero.\n",
    "![](Images/calculo-y3-red-recurrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0b5ff-f659-496c-b4b3-f0100f211de5",
   "metadata": {},
   "source": [
    "se empieza a ver cual es el problema...\n",
    "\n",
    "\n",
    "![](Images/calculo-anidado-y3-red-recurrente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debc868-632c-4ea6-9f4b-e3c983ffdcae",
   "metadata": {},
   "source": [
    "El resultado de esto es que la activación inicial $a_0$ terminará siendo escalada por un valor mucho menor a 1 al llegar a la salida, puesto que las funciones tangente hiperbólica están anidadas y estas tienen un valor que en el mejor de los casos es cercano a uno.\n",
    "\n",
    "**Es decir  el efecto que $a_0$ tendrá en el cálculo de la salida 3 será mínimo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30f741-f3c7-48c0-b827-abd753bb36bb",
   "metadata": {},
   "source": [
    "Esto es lo que hace  que una Red Recurrente básica tenga una memoria de corto plazo: **la secuencia procesada debe ser relativamente corta para que las activaciones anteriores  tenga un efecto relevante en la predicción actual**.\n",
    "\n",
    "\n",
    "\n",
    "![](Images/efecto-escalamiento-red-recurrente.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edbd34-274d-472b-9184-d2552a82ff0f",
   "metadata": {},
   "source": [
    "### Diferencia de LSTM con RNN convencionales\n",
    "\n",
    "Una Red LSTM es capaz de **“recordar”** un dato relevante en la secuencia y de **preservarlo por varios instantes de tiempo**. Por tanto, puede tener una **memoria** tanto de corto plazo (como las Redes Recurrentes básicas) como también de **largo plazo**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99deb7eb-df77-44a8-8a2c-acc1b94de11d",
   "metadata": {},
   "source": [
    "### Celda LSTM\n",
    "Comparado con una celda de red recurrente básica, la celda LSTM tiene una entrada y una salida adicional. Este elemento adicional se conoce como **celda de estado**.\n",
    "\n",
    "\n",
    "![](Images/celda_LSTM.jpg)\n",
    "![](Images/forget_update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b58862-fb79-416a-b281-0fc2b3bd49c0",
   "metadata": {},
   "source": [
    "### Compuertas en LSTM\n",
    "Para añadir o remover datos de esta memoria usamos varias compuertas:\n",
    "1. **Forget gate**: permite eliminar elementos de la memoria.\n",
    "2. **Update gate**:  permite añadir nuevos elementos a la memoria.\n",
    "3. **Compuerta de salida**: permite crear el estado oculto actualizado.\n",
    "\n",
    "![](Images/compuertas-red-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a7a2a-6384-4fe6-9cd3-e1efe706bfd6",
   "metadata": {},
   "source": [
    "**Estas compuertas son redes neuronales que funcionan como válvulas**: totalmente abiertas permiten el paso de información, y totalmente cerradas lo bloquean por completo.\n",
    "\n",
    "Estas compuertas (o válvulas) está conformada por tres elementos: una **red neuronal**, una **función sigmoidal** y un **elemento multiplicador**.\n",
    "\n",
    "\n",
    "![](Images/elementos-compuerta-red-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fffac-c468-4ad9-a004-5b1f8c8339dc",
   "metadata": {},
   "source": [
    "### 1. Compuerta de Olvido (forget gate)\n",
    "- **Objetivo**: permite decidir qué información se va a descartar, y que por tanto no pasará a la celda de estado.\n",
    "- **Cálculo**: \n",
    "  $$\n",
    "  f_t = \\sigma(W_f [a_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "  Donde $\\sigma$ es la función sigmoidal que devuelve valores entre 0 y 1. Valores cercanos a 0 (o iguales a cero) se  eliminaran de la memoria,  mientras que si alcanza valores iguales (o cercanos) a 1 esta información se mantendrá y llegará a la celda de estado..\n",
    "\n",
    "![](Images/compuerta-forget-red-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3181e-5e18-483f-bea4-846c15142305",
   "metadata": {},
   "source": [
    "### 2. Compuerta de Actualización (update gate)\n",
    "- **Objetivo**: Actualiza el estado de la celda añadiendo nueva información relevante.\n",
    "- **Cálculo**: \n",
    "  $$\n",
    "  u_t = \\sigma(W_u [a_{t-1}, x_t] + b_u)\n",
    "  $$\n",
    "  Esta salida pasa por una función sigmoidal, valores cercanos a 1 se consideran para actualizar la memoria.\n",
    "\n",
    "\n",
    "![](Images/compuerta-update-red-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa53de2-ee58-4e8c-b867-365827dfffee",
   "metadata": {},
   "source": [
    "### 3. Generación del Vector Candidato\n",
    "- **Objetivo**: Crear un vector candidato para formar parte del nuevo estado de la celda.\n",
    "- **Cálculo**: \n",
    "  $$\n",
    "  c_t^{'} = \\tanh(W_c [a_{t-1}, x_t] + b_c)\n",
    "  $$\n",
    "  Aquí se usa una función tangente hiperbólica ($\\tanh$) que asegura valores entre $[-1, 1]$.\n",
    "\n",
    "![](Images/actualizacion-celda-estado-red-lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8b467-0663-456a-82ea-c0b2939605d7",
   "metadata": {},
   "source": [
    "### 4. Actualización del Estado de la Celda (cell state)\n",
    "- **Objetivo**: Combinar la información que debe ser olvidada y la nueva información relevante.\n",
    "- **Cálculo**: \n",
    "  $$\n",
    "  c_t = f_t \\cdot c_{t-1} + u_t \\cdot c_t^{'}\n",
    "  $$\n",
    "  Donde $f_t \\cdot c_{t-1}$ elimina información irrelevante y $u_t \\cdot c_t^{'}$ añade nueva información.\n",
    "\n",
    "### 5. Compuerta de Salida (output gate)\n",
    "- **Objetivo**: Decide qué parte del estado de la celda será utilizada para generar el nuevo estado oculto.\n",
    "- **Cálculo**: \n",
    "    En primer lugar **escalamos el nuevo “cell state”** para garantizar que esté en el rango de -1 a 1 (el rango que tiene precisamente el estado oculto). Para ello usamos la **función tangente hiperbólica**. Luego, usamos la compuerta de salida para determinar qué porciones del cell-state entrarán a formar parte del nuevo estado oculto\n",
    "  $$\n",
    "  o_t = \\sigma(W_o [a_{t-1}, x_t] + b_o)\n",
    "  $$\n",
    "  Luego, el nuevo estado oculto $a_t$ se obtiene aplicando la tangente hiperbólica al nuevo estado de la celda $c_t$ y multiplicándolo por la salida de la compuerta:\n",
    "  $$\n",
    "  a_t = o_t \\cdot \\tanh(c_t)\n",
    "  $$\n",
    "\n",
    "![](Images/actualizacion-estado-oculto-red-lstm.png)\n",
    "\n",
    "### En resumen...\n",
    "\n",
    "1. Calcular $f_t = \\sigma(W_f [a_{t-1}, x_t] + b_f)$.\n",
    "2. Calcular $u_t = \\sigma(W_u [a_{t-1}, x_t] + b_u)$.\n",
    "3. Calcular $c_t^{'} = \\tanh(W_c [a_{t-1}, x_t] + b_c)$.\n",
    "4. Actualizar el estado de la celda $c_t = f_t \\cdot c_{t-1} + u_t \\cdot c_t^{'}$.\n",
    "5. Calcular $o_t = \\sigma(W_o [a_{t-1}, x_t] + b_o)$.\n",
    "6. Actualizar el estado oculto $a_t = o_t \\cdot \\tanh(c_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842469b-1e92-4972-b00c-3738a3329a50",
   "metadata": {},
   "source": [
    "### Ventajas de la Red LSTM\n",
    "En esta red  la información puede ser fácilmente removida o añadida de la memoria basta con entrenar adecuadamente las compuertas forget y update, de modo que, con el entrenamiento adecuado, que la información almacenada en el estado C_0 se propague fácilmente hasta el estado C_5 o hasta estados posteriores, y que además la información irrelevante sea eliminada de la memoria en el momento adecuado.\n",
    "\n",
    "![](Images/red-lstm-ventaja-celda-de-estados.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f5a4-b577-4046-9d00-4168c43a5a8f",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30415fa-48e9-4bdc-b1ba-e3451c13cbf5",
   "metadata": {},
   "source": [
    "\n",
    "![](Images/GRU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55d729-d800-481d-877f-947547767bad",
   "metadata": {},
   "source": [
    "Las **GRU** son una variante simplificada de las **LSTM** que permiten manejar dependencias a largo plazo en secuencias.\n",
    "\n",
    "### Similitudes con LSTM:\n",
    "- Ambas son redes recurrentes que permiten manejar secuencias y retener información durante varios pasos.\n",
    "- Utilizan **compuertas** para controlar el flujo de información.\n",
    "\n",
    "### Diferencias clave con LSTM:\n",
    "1. **Menos Compuertas**:  \n",
    "   GRU utiliza **dos compuertas** en lugar de tres:\n",
    "   - **Update Gate**: Decide cuánto del estado anterior mantener.\n",
    "   - **Reset Gate**: Decide cuánta información del pasado olvidar.\n",
    "\n",
    "2. **Estado**:  \n",
    "   GRU no tiene un estado separado de \"celda\" ($c_t$) como en LSTM. En cambio, combina directamente el estado oculto ($h_t$) para ambos propósitos.\n",
    "\n",
    "---\n",
    "\n",
    "## Arquitectura de GRU\n",
    "\n",
    "### 1. Compuerta de actualización (*Update Gate*):\n",
    "\n",
    "Esta compuerta decide cuánto del estado anterior se mantendrá. La fórmula es:\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $z_t$ es el vector de la compuerta de actualización.\n",
    "- $W_z$ son los pesos aprendidos.\n",
    "- $h_{t-1}$ es el estado oculto anterior.\n",
    "- $x_t$ es la entrada en el tiempo $t$.\n",
    "- $\\sigma$ es la función sigmoide.\n",
    "\n",
    "### 2. Compuerta de reseteo (*Reset Gate*):\n",
    "\n",
    "Esta compuerta controla cuánto del estado anterior olvidar. La fórmula es:\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $r_t$ es el vector de la compuerta de reseteo.\n",
    "- $W_r$ son los pesos aprendidos.\n",
    "- $h_{t-1}$ es el estado oculto anterior.\n",
    "- $x_t$ es la entrada en el tiempo $t$.\n",
    "- $\\sigma$ es la función sigmoide.\n",
    "\n",
    "### 3. Cálculo del nuevo estado:\n",
    "\n",
    "El nuevo estado $\\tilde{h}_t$ es un estado candidato, calculado considerando la información más reciente y la información reseteada del pasado:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\cdot h_{t-1}, x_t])\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\tilde{h}_t$ es el estado oculto candidato.\n",
    "- $W_h$ son los pesos aprendidos.\n",
    "- $r_t \\cdot h_{t-1}$ es el estado oculto reseteado.\n",
    "\n",
    "### 4. Actualización del estado oculto:\n",
    "\n",
    "Finalmente, el nuevo estado oculto $h_t$ se calcula como una interpolación entre el estado anterior y el estado candidato:\n",
    "\n",
    "$$\n",
    "h_t = z_t \\cdot h_{t-1} + (1 - z_t) \\cdot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "Esto significa que la compuerta de actualización $z_t$ decide qué porción del estado oculto anterior mantener y qué porción actualizar con el nuevo estado candidato.\n",
    "\n",
    "---\n",
    "\n",
    "## Diferencias con LSTM:\n",
    "- **Menos Compuertas**: GRU no tiene una compuerta de olvido independiente, lo que hace que su cálculo sea más rápido y menos costoso.\n",
    "- **Menos Parámetros**: Al no tener un estado de celda separado ($c_t$), hay menos parámetros y menos complejidad en comparación con las LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62241cdf-950c-4de9-a475-50f92a9684f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
