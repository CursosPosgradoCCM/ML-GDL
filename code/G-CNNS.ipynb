{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bad6536b-fcaa-4b93-b096-970818d2ba39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting e2cnn\n",
      "  Downloading e2cnn-0.2.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch in /home/jupyter-user5/.local/lib/python3.7/site-packages (from e2cnn) (1.13.1+cpu)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/TDA/lib/python3.7/site-packages (from e2cnn) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/TDA/lib/python3.7/site-packages (from e2cnn) (1.7.3)\n",
      "Collecting sympy (from e2cnn)\n",
      "  Downloading sympy-1.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from sympy->e2cnn) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch->e2cnn) (4.4.0)\n",
      "Downloading e2cnn-0.2.3-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, e2cnn\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jupyter-user5/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed e2cnn-0.2.3 sympy-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install e2cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559eb38a-b3c7-40d4-be17-a5f8d1613282",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.17.tar.gz (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 36, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-wjlimfuq/torch-sparse_b90b9464a89d4a519b46b253303069cd/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torch-geometric torch-sparse torch-scatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbf694f-e8c1-41ab-98e7-886c3f8eff2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (199.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.14.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-0.13.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torchvision) (2024.8.30)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter-user5/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1+cpu torchaudio-0.13.1+cpu torchvision-0.14.1+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf95f38-f4bb-4d7a-a196-70bf263ca56d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/jupyter-user5/.local/lib/python3.7/site-packages (24.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/TDA/lib/python3.7/site-packages (65.6.3)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/TDA/lib/python3.7/site-packages (0.37.1)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/home/jupyter-user5/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed setuptools-68.0.0 wheel-0.42.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f8aecf-a0ba-479f-88ab-215279dd6471",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m786.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Using cached torch_sparse-0.6.17.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Downloading torch_cluster-1.6.1.tar.gz (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-spline-conv\n",
      "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-geometric\n",
      "  Using cached torch_geometric-2.3.1.tar.gz (661 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-sparse) (1.7.3)\n",
      "Requirement already satisfied: tqdm in /home/jupyter-user5/.local/lib/python3.7/site-packages (from torch-geometric) (4.66.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (1.21.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from torch-geometric) (5.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/TDA/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv, torch-geometric\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-scatter: filename=torch_scatter-2.1.1-cp37-cp37m-linux_x86_64.whl size=485258 sha256=0772506bbf08332579a2e6c385a99b003973bb5f6e63f024cc0ad836754f4a28\n",
      "  Stored in directory: /home/jupyter-user5/.cache/pip/wheels/4f/67/e1/27db5e63a3cda9d1237b50015fa2e7846711d65888ce83ba2f\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-sparse: filename=torch_sparse-0.6.17-cp37-cp37m-linux_x86_64.whl size=1033449 sha256=1b1beeb615978055c6720e29f836834f036e7986a5c58b4e8e71360821816ffe\n",
      "  Stored in directory: /home/jupyter-user5/.cache/pip/wheels/db/51/fe/3abf5dda2cbc79ece62720bf9ff71a56626ff510bc59d5cfa1\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-cluster: filename=torch_cluster-1.6.1-cp37-cp37m-linux_x86_64.whl size=699502 sha256=2641dc410d7d366bddd60a503c2a1370cebc65dd38138174baf7eef1391a8478\n",
      "  Stored in directory: /home/jupyter-user5/.cache/pip/wheels/a2/44/33/8651ab2ce799fb175216388316f5bfd5ee5ca7e9728d1191b6\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp37-cp37m-linux_x86_64.whl size=198986 sha256=d5c072a1528c396e76730a706ff9e3576c678be0454fc9f86a55b67d87b589d2\n",
      "  Stored in directory: /home/jupyter-user5/.cache/pip/wheels/89/a1/08/0d083f90c923f9e650977718fce0c973cc99a5415102078e24\n",
      "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910453 sha256=e1dd0036c7b1d7b21c1befd9b79bd3a91299cdb2f585c4fba4de285ab816122f\n",
      "  Stored in directory: /home/jupyter-user5/.cache/pip/wheels/0b/ad/62/8bc3dda54b069f0b7072a64fba0c48f5cdb1572daf2e6ceaa3\n",
      "Successfully built torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric\n",
      "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
      "Successfully installed torch-cluster-1.6.1 torch-geometric-2.3.1 torch-scatter-2.1.1 torch-sparse-0.6.17 torch-spline-conv-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3663f-8d73-4297-9b93-cec71311711d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Descripción de Bibliotecas para Aprendizaje Profundo\n",
    "\n",
    "## **1. `torch`**\n",
    "La biblioteca base de PyTorch, utilizada para:\n",
    "- Crear y manejar **tensores** (estructuras multidimensionales optimizadas para GPU).\n",
    "- Realizar cálculos matemáticos en CPU o GPU.\n",
    "- Definir y entrenar modelos de aprendizaje profundo.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. `torch.nn`**\n",
    "Herramientas para construir redes neuronales:\n",
    "- **Capas predefinidas**: `Linear`, `Conv2d`, `LSTM`, etc.\n",
    "- **Funciones de pérdida**: `CrossEntropyLoss`, `MSELoss`.\n",
    "- **Optimizadores** para actualizar pesos durante el entrenamiento.\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "layer = nn.Linear(in_features=128, out_features=64)\n",
    "\n",
    "---\n",
    "\n",
    "## **3. `torch.nn.functional` (F)**\n",
    "Proporciona funciones específicas para redes neuronales, como:\n",
    "- Funciones de activación: `F.relu()`.\n",
    "- Operaciones matemáticas directamente sobre tensores.\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "output = F.relu(input_tensor)\n",
    "\n",
    "## **4. `torch.utils.data.DataLoader`**\n",
    "Carga y organiza los datos de forma eficiente:\n",
    "- Divide los datos en **minibatches**.\n",
    "- Mezcla los datos (shuffling).\n",
    "- Facilita el manejo de grandes datasets.\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "## **5. `torchvision.datasets`**\n",
    "Incluye **datasets predefinidos** para tareas de visión por computadora:\n",
    "- MNIST, CIFAR-10, ImageNet.\n",
    "- Descarga y carga sencilla de datasets.\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "## **6. `torchvision.transforms`**\n",
    "Proporciona **transformaciones comunes** para preprocesar imágenes:\n",
    "- Conversión a tensores: `ToTensor`.\n",
    "- Normalización: `Normalize`.\n",
    "- Aumentación de datos: `RandomCrop`, `RandomRotation`.\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca846eec-9bbf-4859-a98a-b4a6ddd8eec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "711a3fa2-9ccc-4061-bb80-19837d47c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd888d93-2e2a-4593-8f6d-78ecbe96d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para normalizar las imágenes de MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizar entre -1 y 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab9670f6-277d-4904-9d89-f523dde1e824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1428057ae5074f70a0e2228458c6bef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fee05e2e2848919eda40b9c738359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7564def9781b4f7a9508f36af153cf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cab398c6694c23b576648da12b57ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.MNIST(root=\"data\", train=True, transform=transform, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a20a18-68d1-408d-9474-21e060385eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = mnist[0]\n",
    "\n",
    "# Convertir la imagen a un formato adecuado para mostrarla (de tensor a numpy array)\n",
    "image = image.numpy().squeeze()  # Remove the channel dimension (1, 28, 28) to (28, 28)\n",
    "\n",
    "# Mostrar la imagen usando matplotlib\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Etiqueta: {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd49b30-1854-4736-8d64-c78b1d95c079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mostrar_lote_de_imagenes(n=10):\n",
    "    fig, axes = plt.subplots(1, n, figsize=(15, 15))\n",
    "    for i in range(n):\n",
    "        image, label = mnist[i]\n",
    "        axes[i].imshow(image.numpy().squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f'Etiqueta: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "mostrar_lote_de_imagenes(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4978be-95ea-4f52-8705-6186b2c62c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cee85-4711-4ea8-80d5-3a282cefbecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207877b8-04b2-42fb-9b84-2b682a77e96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformaciones para normalizar las imágenes de MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizar entre -1 y 1\n",
    "])\n",
    "\n",
    "# Cargar los datos de entrenamiento y prueba de MNIST\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Preparar el DataLoader para iterar sobre los datos en lotes\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Visualizar algunas imágenes de MNIST\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(example_data[i][0], cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {example_targets[i].item()}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b00a6-1bb6-4314-a250-1d54d796dc32",
   "metadata": {},
   "source": [
    "Con esto, cargamos y visualizamos algunas imágenes de MNIST para entender mejor la entrada al modelo. Cada imagen es de 28x28 píxeles en escala de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38a1fe32-4844-4e73-861c-cdc0dfd1f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiftingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LiftingLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Crear 4 rotaciones: 0°, 90°, 180°, y 270°\n",
    "        x0 = x\n",
    "        x90 = torch.rot90(x, 1, [2, 3])\n",
    "        x180 = torch.rot90(x, 2, [2, 3])\n",
    "        x270 = torch.rot90(x, 3, [2, 3])\n",
    "        # Concatenar rotaciones en la dimensión de canales\n",
    "        return torch.cat([x0, x90, x180, x270], dim=1)\n",
    "\n",
    "class GroupConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GroupConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class GroupReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.relu(x)\n",
    "\n",
    "class GroupPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "class FinalPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Agrupamiento global\n",
    "        return F.adaptive_max_pool2d(x, (1, 1)).view(x.size(0), -1)\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNClassifier, self).__init__()\n",
    "        self.lifting = LiftingLayer()\n",
    "        self.group_conv1 = GroupConv(4, 16)\n",
    "        self.group_conv2 = GroupConv(16, 32)\n",
    "        self.group_relu = GroupReLU()\n",
    "        self.group_pooling = GroupPooling()\n",
    "        self.final_pooling = FinalPooling()\n",
    "        self.fc = nn.Linear(32, 10)  # 10 clases para MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lifting(x)\n",
    "        x = self.group_conv1(x)\n",
    "        x = self.group_relu(x)\n",
    "        x = self.group_pooling(x)\n",
    "        x = self.group_conv2(x)\n",
    "        x = self.group_relu(x)\n",
    "        x = self.group_pooling(x)\n",
    "        x = self.final_pooling(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5d2d-6cb8-4be0-9978-12bcc333e024",
   "metadata": {},
   "source": [
    "Esta arquitectura toma una imagen, la rota en 4 ángulos y aplica convoluciones sobre esas versiones. Luego, se reduce la dimensionalidad usando pooling, y finalmente una capa totalmente conectada da la salida de la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6cd83-55c2-48fd-ae59-40cee1d6f85f",
   "metadata": {},
   "source": [
    "LiftingLayer: Realiza rotaciones para aumentar la diversidad de las imágenes de entrada.\n",
    "GroupConv: Convolución 2D que aplica filtros a la imagen.\n",
    "GroupReLU: Función de activación que introduce no linealidades.\n",
    "GroupPooling: Reducción de resolución mediante max pooling.\n",
    "FinalPooling: Reducción global de las características a un solo valor por canal.\n",
    "GCNClassifier: La red neuronal que combina todas las capas anteriores para clasificar las imágenes en 10 clases (para MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5253883a-46c9-41ff-bf28-d0a3cd179761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6406\n",
      "Epoch [2/5], Loss: 0.2248\n",
      "Epoch [3/5], Loss: 0.1755\n",
      "Epoch [4/5], Loss: 0.1456\n",
      "Epoch [5/5], Loss: 0.1288\n"
     ]
    }
   ],
   "source": [
    "# Configuración de entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Adelante y atrás\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2be1442-036a-4564-b3fe-bb941c359b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 95.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en el conjunto de prueba\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5178cbc-0018-4df9-b663-a207f977938b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d50d968-b82c-4209-80d5-0af1f8972eef",
   "metadata": {},
   "source": [
    "CNN BASICA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027edde0-f2f5-4fc1-8f1d-82ad02a149dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2c7cee5-b78e-4285-a09c-1166312f12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 clases para MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Aplanar\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f98b21e0-1264-44b6-967f-a8298ba30680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2027\n",
      "Epoch [2/5], Loss: 0.0563\n",
      "Epoch [3/5], Loss: 0.0385\n",
      "Epoch [4/5], Loss: 0.0296\n",
      "Epoch [5/5], Loss: 0.0232\n"
     ]
    }
   ],
   "source": [
    "# Configuración de entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BasicCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Adelante y atrás\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "689a7e9f-2c4f-4c90-9932-38f0edf0a748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Basic CNN on the test images: 98.99%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en el conjunto de prueba\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the Basic CNN on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef1ed6-1cb2-470d-92b9-fd9ba804b49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df6ea9-75db-4c79-aee1-9c076374fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3911a98-3a7c-476c-9c11-6b4903be3541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd7d7e6-3dc2-4cda-9e19-931011be5ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio de entrenamiento:\n",
      "['notumor', 'pituitary', 'meningioma', 'glioma', '.ipynb_checkpoints']\n",
      "\n",
      "Subcarpetas en 'Training':\n",
      "notumor\n",
      "pituitary\n",
      "meningioma\n",
      "glioma\n",
      ".ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Archivos en el directorio de entrenamiento:\")\n",
    "print(os.listdir(train_dir))\n",
    "\n",
    "print(\"\\nSubcarpetas en 'Training':\")\n",
    "for folder in os.listdir(train_dir):\n",
    "    print(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b225f3c-5215-4348-a9b7-8ab359fc6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando carpeta: /home/jupyter-user5/Clase-2025-1/data/Training/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Eliminar .ipynb_checkpoints de los subdirectorios\n",
    "for root, dirs, files in os.walk(train_dir):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name == \".ipynb_checkpoints\":\n",
    "            folder_to_delete = os.path.join(root, dir_name)\n",
    "            print(f\"Eliminando carpeta: {folder_to_delete}\")\n",
    "            os.rmdir(folder_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02af2a92-3019-4cb6-a428-f4cc626a1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases encontradas: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Cantidad de imágenes de entrenamiento: 3935\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformaciones básicas\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargar los datos ignorando `.ipynb_checkpoints`\n",
    "train_data = datasets.ImageFolder(\n",
    "    root=train_dir, \n",
    "    transform=transform,\n",
    "    is_valid_file=lambda x: x.endswith(('.jpg', '.jpeg', '.png'))\n",
    ")\n",
    "\n",
    "print(f\"Clases encontradas: {train_data.classes}\")\n",
    "print(f\"Cantidad de imágenes de entrenamiento: {len(train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e1e7c49-7f1b-42dc-b0e1-43d58fa5638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases encontradas: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Número de imágenes en el conjunto de entrenamiento: 3935\n"
     ]
    }
   ],
   "source": [
    "print(f\"Clases encontradas: {train_data.classes}\")\n",
    "print(f\"Número de imágenes en el conjunto de entrenamiento: {len(train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e938cd-02d3-4b3b-bb84-492baa823523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de entrenamiento...\n",
      "Cargando datos de prueba...\n",
      "\n",
      "Clases detectadas: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Tamaño del conjunto de entrenamiento: 3935 imágenes\n",
      "Tamaño del conjunto de prueba: 1311 imágenes\n",
      "\n",
      "Distribución de clases en el conjunto de entrenamiento:\n",
      "Clase 'glioma': 1321 imágenes\n",
      "Clase 'meningioma': 1339 imágenes\n",
      "Clase 'notumor': 795 imágenes\n",
      "Clase 'pituitary': 480 imágenes\n",
      "\n",
      "Distribución de clases en el conjunto de prueba:\n",
      "Clase 'glioma': 300 imágenes\n",
      "Clase 'meningioma': 306 imágenes\n",
      "Clase 'notumor': 405 imágenes\n",
      "Clase 'pituitary': 300 imágenes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Directorios\n",
    "train_dir = '/home/jupyter-user5/Clase-2025-1/data/Training'\n",
    "test_dir = '/home/jupyter-user5/Clase-2025-1/data/Testing'\n",
    "\n",
    "# Primero, eliminar la carpeta .ipynb_checkpoints del directorio Testing\n",
    "checkpoints_dir = os.path.join(test_dir, '.ipynb_checkpoints')\n",
    "if os.path.exists(checkpoints_dir):\n",
    "    print(f\"Eliminando directorio: {checkpoints_dir}\")\n",
    "    import shutil\n",
    "    shutil.rmtree(checkpoints_dir)\n",
    "\n",
    "# Cargar los datos\n",
    "print(\"Cargando datos de entrenamiento...\")\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "print(\"Cargando datos de prueba...\")\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Crear los dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Mostrar información\n",
    "classes = train_data.classes\n",
    "print(f\"\\nClases detectadas: {classes}\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_data)} imágenes\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(test_data)} imágenes\")\n",
    "\n",
    "# Distribución de clases en el conjunto de entrenamiento\n",
    "print(\"\\nDistribución de clases en el conjunto de entrenamiento:\")\n",
    "for i, clase in enumerate(classes):\n",
    "    count = sum(1 for _, label in train_data if label == i)\n",
    "    print(f\"Clase '{clase}': {count} imágenes\")\n",
    "\n",
    "print(\"\\nDistribución de clases en el conjunto de prueba:\")\n",
    "for i, clase in enumerate(classes):\n",
    "    count = sum(1 for _, label in test_data if label == i)\n",
    "    print(f\"Clase '{clase}': {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534af7a3-d3f6-4b6d-bfd9-0307f5b4b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando en: cpu\n",
      "Epoch [1/5], Step [10/123], Loss: 1.1425, Accuracy: 43.12%\n",
      "Epoch [1/5], Step [20/123], Loss: 0.8242, Accuracy: 50.47%\n",
      "Epoch [1/5], Step [30/123], Loss: 0.9351, Accuracy: 55.10%\n",
      "Epoch [1/5], Step [40/123], Loss: 0.8170, Accuracy: 58.05%\n",
      "Epoch [1/5], Step [50/123], Loss: 0.7198, Accuracy: 59.12%\n",
      "Epoch [1/5], Step [60/123], Loss: 0.9276, Accuracy: 60.57%\n",
      "Epoch [1/5], Step [70/123], Loss: 0.8482, Accuracy: 61.83%\n",
      "Epoch [1/5], Step [80/123], Loss: 0.5608, Accuracy: 63.67%\n",
      "Epoch [1/5], Step [90/123], Loss: 0.6859, Accuracy: 64.03%\n",
      "Epoch [1/5], Step [100/123], Loss: 0.6223, Accuracy: 64.91%\n",
      "Epoch [1/5], Step [110/123], Loss: 0.4769, Accuracy: 66.02%\n",
      "Epoch [1/5], Step [120/123], Loss: 0.5338, Accuracy: 66.72%\n",
      "\n",
      "Epoch 1 - Loss: 0.7944, Training Accuracy: 66.94%\n",
      "Test Accuracy: 73.00%\n",
      "\n",
      "Epoch [2/5], Step [10/123], Loss: 0.4253, Accuracy: 79.69%\n",
      "Epoch [2/5], Step [20/123], Loss: 0.8160, Accuracy: 78.91%\n",
      "Epoch [2/5], Step [30/123], Loss: 0.6290, Accuracy: 78.02%\n",
      "Epoch [2/5], Step [40/123], Loss: 0.4288, Accuracy: 78.28%\n",
      "Epoch [2/5], Step [50/123], Loss: 0.3148, Accuracy: 78.25%\n",
      "Epoch [2/5], Step [60/123], Loss: 0.3948, Accuracy: 78.54%\n",
      "Epoch [2/5], Step [70/123], Loss: 0.4983, Accuracy: 78.57%\n",
      "Epoch [2/5], Step [80/123], Loss: 0.6086, Accuracy: 79.18%\n",
      "Epoch [2/5], Step [90/123], Loss: 0.6595, Accuracy: 79.34%\n",
      "Epoch [2/5], Step [100/123], Loss: 0.3661, Accuracy: 79.38%\n",
      "Epoch [2/5], Step [110/123], Loss: 0.2792, Accuracy: 79.69%\n",
      "Epoch [2/5], Step [120/123], Loss: 0.5536, Accuracy: 80.10%\n",
      "\n",
      "Epoch 2 - Loss: 0.4889, Training Accuracy: 80.13%\n",
      "Test Accuracy: 77.57%\n",
      "\n",
      "Epoch [3/5], Step [10/123], Loss: 0.3931, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [20/123], Loss: 0.3119, Accuracy: 81.72%\n",
      "Epoch [3/5], Step [30/123], Loss: 0.4146, Accuracy: 81.98%\n",
      "Epoch [3/5], Step [40/123], Loss: 0.5160, Accuracy: 82.19%\n",
      "Epoch [3/5], Step [50/123], Loss: 0.3322, Accuracy: 82.19%\n",
      "Epoch [3/5], Step [60/123], Loss: 0.3252, Accuracy: 83.02%\n",
      "Epoch [3/5], Step [70/123], Loss: 0.1891, Accuracy: 83.75%\n",
      "Epoch [3/5], Step [80/123], Loss: 0.3743, Accuracy: 83.32%\n",
      "Epoch [3/5], Step [90/123], Loss: 0.5307, Accuracy: 83.40%\n",
      "Epoch [3/5], Step [100/123], Loss: 0.5219, Accuracy: 83.16%\n",
      "Epoch [3/5], Step [110/123], Loss: 0.6978, Accuracy: 83.55%\n",
      "Epoch [3/5], Step [120/123], Loss: 0.2573, Accuracy: 83.88%\n",
      "\n",
      "Epoch 3 - Loss: 0.4042, Training Accuracy: 83.94%\n",
      "Test Accuracy: 83.83%\n",
      "\n",
      "Epoch [4/5], Step [10/123], Loss: 0.1723, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [20/123], Loss: 0.1909, Accuracy: 86.88%\n",
      "Epoch [4/5], Step [30/123], Loss: 0.2259, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [40/123], Loss: 0.1963, Accuracy: 88.75%\n",
      "Epoch [4/5], Step [50/123], Loss: 0.2415, Accuracy: 88.69%\n",
      "Epoch [4/5], Step [60/123], Loss: 0.2907, Accuracy: 88.65%\n",
      "Epoch [4/5], Step [70/123], Loss: 0.1481, Accuracy: 88.93%\n",
      "Epoch [4/5], Step [80/123], Loss: 0.2999, Accuracy: 88.79%\n",
      "Epoch [4/5], Step [90/123], Loss: 0.2750, Accuracy: 88.89%\n",
      "Epoch [4/5], Step [100/123], Loss: 0.1551, Accuracy: 88.84%\n",
      "Epoch [4/5], Step [110/123], Loss: 0.2731, Accuracy: 88.78%\n",
      "Epoch [4/5], Step [120/123], Loss: 0.1911, Accuracy: 88.70%\n",
      "\n",
      "Epoch 4 - Loss: 0.2821, Training Accuracy: 88.69%\n",
      "Test Accuracy: 89.47%\n",
      "\n",
      "Epoch [5/5], Step [10/123], Loss: 0.1302, Accuracy: 91.88%\n",
      "Epoch [5/5], Step [20/123], Loss: 0.1663, Accuracy: 91.41%\n",
      "Epoch [5/5], Step [30/123], Loss: 0.1112, Accuracy: 91.98%\n",
      "Epoch [5/5], Step [40/123], Loss: 0.1267, Accuracy: 91.95%\n",
      "Epoch [5/5], Step [50/123], Loss: 0.2263, Accuracy: 91.88%\n",
      "Epoch [5/5], Step [60/123], Loss: 0.1228, Accuracy: 92.03%\n",
      "Epoch [5/5], Step [70/123], Loss: 0.3413, Accuracy: 92.28%\n",
      "Epoch [5/5], Step [80/123], Loss: 0.3297, Accuracy: 92.23%\n",
      "Epoch [5/5], Step [90/123], Loss: 0.1421, Accuracy: 92.26%\n",
      "Epoch [5/5], Step [100/123], Loss: 0.2136, Accuracy: 92.22%\n",
      "Epoch [5/5], Step [110/123], Loss: 0.2242, Accuracy: 92.16%\n",
      "Epoch [5/5], Step [120/123], Loss: 0.2320, Accuracy: 92.21%\n",
      "\n",
      "Epoch 5 - Loss: 0.2100, Training Accuracy: 92.25%\n",
      "Test Accuracy: 90.01%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir la arquitectura de una CNN básica\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Cambiar el primer canal de entrada a 3 para imágenes RGB\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, len(classes))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)  # Aplanar\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Entrenando en: {device}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Barra de progreso para el entrenamiento\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calcular accuracy durante el entrenamiento\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:  # Mostrar cada 10 batches\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Accuracy: {100 * correct/total:.2f}%')\n",
    "        \n",
    "        epoch_loss = running_loss/len(train_loader)\n",
    "        epoch_acc = 100 * correct/total\n",
    "        print(f\"\\nEpoch {epoch+1} - Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Evaluación después de cada época\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\\n\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "cnn_model = SimpleCNN()\n",
    "cnn_model = train_and_evaluate(cnn_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eca38-6568-4b86-bfa3-22853b9807e8",
   "metadata": {},
   "source": [
    "Para la G-CNN, usaremos el paquete e2cnn, que permite implementar redes convolucionales que son equivariantes a grupos como rotaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc1d44-9a72-40fd-908f-17e622a32a08",
   "metadata": {},
   "source": [
    "El submódulo **gspaces** permite definir espacios de simetría geométrica (geometric spaces).\n",
    "    * Define las transformaciones que la red manejará como simétricas (rotaciones, reflexiones, traslaciones, etc.).\n",
    "        * Flip2dOnR2: Simetrías que incluyen reflexiones.\n",
    "        * TrivialOnR2: Sin simetrías adicionales, equivalente a una CNN estándar.\n",
    "\n",
    "El submódulo nn define las capas convolucionales equivariantes y otras operaciones que respetan las simetrías del espacio geométrico.\n",
    "    * Funciona junto con gspaces para implementar redes que son invariantes o equivariantes a ciertas transformaciones.\n",
    "        * enn.R2Conv: Realiza convoluciones equivariantes.\n",
    "        * Similar a nn.Conv2d de PyTorch, pero respetando simetrías.\n",
    "        * enn.GatedNonLinearity: Aplicación de no linealidades en espacios equivariantes.\n",
    "        * enn.PointwiseAvgPool: Pooling diseñado para mantener propiedades de simetría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1d73fb-b264-4850-88a7-14621d436c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7956\n",
      "Epoch [2/10], Loss: 0.4939\n",
      "Epoch [3/10], Loss: 0.3910\n",
      "Epoch [4/10], Loss: 0.3285\n",
      "Epoch [5/10], Loss: 0.2704\n",
      "Epoch [6/10], Loss: 0.2244\n",
      "Epoch [7/10], Loss: 0.1934\n",
      "Epoch [8/10], Loss: 0.1703\n",
      "Epoch [9/10], Loss: 0.1372\n",
      "Epoch [10/10], Loss: 0.1346\n",
      "Accuracy: 90.62%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# 1. Configuración del espacio de simetrías\n",
    "# Grupo de rotaciones discretas C4 (0°, 90°, 180°, 270°)\n",
    "gspace = gspaces.Rot2dOnR2(N=4)\n",
    "\n",
    "# 2. Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        # Primer capa convolucional G-CNN\n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        \n",
    "        # Segunda capa convolucional G-CNN\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        \n",
    "        # Pooling equivariante\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)  # Reduce al espacio usual\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 4)  # Ajusta tamaño según dimensiones\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor  # Volver al espacio usual\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),  # Convertir a escala de grises si es necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalización\n",
    "])\n",
    "# Directorios\n",
    "train_dir = '/home/jupyter-user5/Clase-2025-1/data/Training'\n",
    "test_dir = '/home/jupyter-user5/Clase-2025-1/data/Testing'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward y backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Evaluación\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "502bdf06-07a5-4341-b61a-a2ffabe4969f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8234\n",
      "Epoch [2/10], Loss: 0.5221\n",
      "Epoch [3/10], Loss: 0.4618\n",
      "Epoch [4/10], Loss: 0.3602\n",
      "Epoch [5/10], Loss: 0.2945\n",
      "Epoch [6/10], Loss: 0.2589\n",
      "Epoch [7/10], Loss: 0.2371\n",
      "Epoch [8/10], Loss: 0.1937\n",
      "Epoch [9/10], Loss: 0.1695\n",
      "Epoch [10/10], Loss: 0.1465\n",
      "Accuracy: 89.47%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# 1. Configuración del espacio de simetrías\n",
    "# Grupo de rotaciones discretas C4 (0°, 90°, 180°, 270°)\n",
    "gspace = gspaces.Rot2dOnR2(N=8)\n",
    "\n",
    "# 2. Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        # Primer capa convolucional G-CNN\n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        \n",
    "        # Segunda capa convolucional G-CNN\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        \n",
    "        # Pooling equivariante\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)  # Reduce al espacio usual\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 4)  # Ajusta tamaño según dimensiones\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor  # Volver al espacio usual\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),  # Convertir a escala de grises si es necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalización\n",
    "])\n",
    "# Directorios\n",
    "train_dir = '/home/jupyter-user5/Clase-2025-1/data/Training'\n",
    "test_dir = '/home/jupyter-user5/Clase-2025-1/data/Testing'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward y backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Evaluación\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d46cdff-7f3b-437b-9270-6ddc1dbb0011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8039\n",
      "Epoch [2/10], Loss: 0.5119\n",
      "Epoch [3/10], Loss: 0.4317\n",
      "Epoch [4/10], Loss: 0.3450\n",
      "Epoch [5/10], Loss: 0.2983\n",
      "Epoch [6/10], Loss: 0.2707\n",
      "Epoch [7/10], Loss: 0.2185\n",
      "Epoch [8/10], Loss: 0.2083\n",
      "Epoch [9/10], Loss: 0.1808\n",
      "Epoch [10/10], Loss: 0.1500\n",
      "Accuracy: 88.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# 1. Configuración del espacio de simetrías\n",
    "# Grupo de rotaciones discretas C4 (0°, 90°, 180°, 270°)\n",
    "gspace = gspaces.Rot2dOnR2(N=16)\n",
    "\n",
    "# 2. Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        # Primer capa convolucional G-CNN\n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        \n",
    "        # Segunda capa convolucional G-CNN\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        \n",
    "        # Pooling equivariante\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)  # Reduce al espacio usual\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 4)  # Ajusta tamaño según dimensiones\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor  # Volver al espacio usual\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),  # Convertir a escala de grises si es necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalización\n",
    "])\n",
    "# Directorios\n",
    "train_dir = '/home/jupyter-user5/Clase-2025-1/data/Training'\n",
    "test_dir = '/home/jupyter-user5/Clase-2025-1/data/Testing'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward y backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Evaluación\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25661cf9-c3f1-4050-8093-69507f4b1f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7770\n",
      "Epoch [2/10], Loss: 0.4856\n",
      "Epoch [3/10], Loss: 0.4021\n",
      "Epoch [4/10], Loss: 0.3537\n",
      "Epoch [5/10], Loss: 0.3014\n",
      "Epoch [6/10], Loss: 0.2540\n",
      "Epoch [7/10], Loss: 0.2167\n",
      "Epoch [8/10], Loss: 0.1898\n",
      "Epoch [9/10], Loss: 0.1687\n",
      "Epoch [10/10], Loss: 0.1344\n",
      "Accuracy: 88.41%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# 1. Configuración del espacio de simetrías\n",
    "# Grupo de rotaciones discretas C4 (0°, 90°, 180°, 270°)\n",
    "gspace = gspaces.Rot2dOnR2(N=2)\n",
    "\n",
    "# 2. Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        # Primer capa convolucional G-CNN\n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        \n",
    "        # Segunda capa convolucional G-CNN\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        \n",
    "        # Pooling equivariante\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Capa totalmente conectada\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)  # Reduce al espacio usual\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 4)  # Ajusta tamaño según dimensiones\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor  # Volver al espacio usual\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),  # Convertir a escala de grises si es necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalización\n",
    "])\n",
    "# Directorios\n",
    "train_dir = '/home/jupyter-user5/Clase-2025-1/data/Training'\n",
    "test_dir = '/home/jupyter-user5/Clase-2025-1/data/Testing'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward y backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Evaluación\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c5765-0b04-41ee-b806-4524bbf659ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7284f27-9293-48f3-a29e-beffbdb196ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56c028-d615-45a9-8982-ff3a5e363a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cca2d31-9df2-425f-bf18-8b8a4795aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc0fe6f-2fc7-484a-9c9e-85808f35e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clasificar imágenes de un conjunto de datos que contiene figuras geométricas simples (cuadrados, triángulos y círculos). \n",
    "#La red aprovechará las simetrías de rotación para mejorar su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65d24436-9b01-4cbd-94ac-87d40dbddd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera un conjunto de imágenes de figuras geométricas (cuadrados, triángulos y círculos) rotadas en ángulos aleatorios (0°, 90°, 180°, 270°).\n",
    "#Usa una librería como matplotlib para generar las imágenes y torchvision.datasets.ImageFolder para cargar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efafcb-41ae-46af-9b3b-3808f3299c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30eb94ee-528d-4441-91b2-18b96ce49932",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "def generate_shapes(output_dir, num_images=300, img_size=(28, 28)):\n",
    "    shapes = ['circle', 'square', 'triangle']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for shape in shapes:\n",
    "        shape_dir = os.path.join(output_dir, shape)\n",
    "        os.makedirs(shape_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            fig, ax = plt.subplots(figsize=(1, 1))\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if shape == 'circle':\n",
    "                circle = plt.Circle((0.5, 0.5), 0.3, color='black')\n",
    "                ax.add_patch(circle)\n",
    "            elif shape == 'square':\n",
    "                square = plt.Rectangle((0.2, 0.2), 0.6, 0.6, color='black')\n",
    "                ax.add_patch(square)\n",
    "            elif shape == 'triangle':\n",
    "                triangle = plt.Polygon([[0.5, 0.8], [0.2, 0.2], [0.8, 0.2]], color='black')\n",
    "                ax.add_patch(triangle)\n",
    "            \n",
    "            angle = np.random.choice([0, 90, 180, 270])\n",
    "            plt.savefig('temp.png', bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            img = Image.open('temp.png').rotate(angle, expand=True).convert('L')\n",
    "            img = img.resize(img_size)\n",
    "            img.save(os.path.join(shape_dir, f\"{shape}_{i}.png\"))\n",
    "    \n",
    "    os.remove('temp.png')\n",
    "\n",
    "# Generar las imágenes\n",
    "output_dir = \"shapes_dataset\"\n",
    "generate_shapes(output_dir)\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "train_dir = \"shapes_train\"\n",
    "test_dir = \"shapes_test\"\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "for shape in os.listdir(output_dir):\n",
    "    shape_dir = os.path.join(output_dir, shape)\n",
    "    files = os.listdir(shape_dir)\n",
    "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    os.makedirs(os.path.join(train_dir, shape), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, shape), exist_ok=True)\n",
    "    \n",
    "    for f in train_files:\n",
    "        shutil.move(os.path.join(shape_dir, f), os.path.join(train_dir, shape, f))\n",
    "    for f in test_files:\n",
    "        shutil.move(os.path.join(shape_dir, f), os.path.join(test_dir, shape, f))\n",
    "\n",
    "shutil.rmtree(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8f531-e7e1-4682-b924-88faa04dc4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fe15f99-8f62-4aeb-82a0-77b3ff5dcf25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# Configuración del espacio de simetrías\n",
    "gspace = gspaces.Rot2dOnR2(N=4)\n",
    "\n",
    "# Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6dbbf5c-0fbe-4cd6-9a8a-f6fddf5e88d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3114\n",
      "Epoch [2/10], Loss: 0.0001\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Configuración y entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c677ee25-bd1b-4158-992c-93a46d001277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of G-CNN: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of G-CNN: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9ec5e6-3386-4ab1-9d70-8e5f7aff35f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c5226eb-ac60-4f41-bbb7-573ab3bcd0d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2992\n",
      "Epoch [2/10], Loss: 0.0003\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Configuración y entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27addc7c-ec16-4702-abce-227ac5a93942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of CNN: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd782b98-5154-4949-a78d-3460628c1a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45142d28-25ca-4e17-a546-57f8f430fcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c37176-8586-4b2a-99ba-bc7b0e45df08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3aacfe0e-7659-4851-b647-c70a625b6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos con ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa35a139-c9da-4ec5-adb6-a03ad4a8c200",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_shapes_with_noise(output_dir, num_images=300, img_size=(28, 28)):\n",
    "    shapes = ['circle', 'square', 'triangle']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for shape in shapes:\n",
    "        shape_dir = os.path.join(output_dir, shape)\n",
    "        os.makedirs(shape_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            fig, ax = plt.subplots(figsize=(1, 1))\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if shape == 'circle':\n",
    "                circle = plt.Circle((0.5, 0.5), 0.3, color='black')\n",
    "                ax.add_patch(circle)\n",
    "            elif shape == 'square':\n",
    "                square = plt.Rectangle((0.2, 0.2), 0.6, 0.6, color='black')\n",
    "                ax.add_patch(square)\n",
    "            elif shape == 'triangle':\n",
    "                triangle = plt.Polygon([[0.5, 0.8], [0.2, 0.2], [0.8, 0.2]], color='black')\n",
    "                ax.add_patch(triangle)\n",
    "            \n",
    "            angle = np.random.uniform(0, 360)  # Rotación continua\n",
    "            plt.savefig('temp.png', bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            img = Image.open('temp.png').rotate(angle, expand=True).convert('L')\n",
    "            img = img.resize(img_size)\n",
    "            \n",
    "            # Añadir ruido gaussiano\n",
    "            img = np.array(img)\n",
    "            noise = np.random.normal(0, 25, img.shape)  # Media=0, Desviación estándar=25\n",
    "            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "            Image.fromarray(img).save(os.path.join(shape_dir, f\"{shape}_{i}.png\"))\n",
    "    \n",
    "    os.remove('temp.png')\n",
    "\n",
    "# Generar datos\n",
    "output_dir = \"shapes_with_noise\"\n",
    "generate_shapes_with_noise(output_dir)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba (igual que antes)\n",
    "train_dir = \"shapes_train_noise\"\n",
    "test_dir = \"shapes_test_noise\"\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "for shape in os.listdir(output_dir):\n",
    "    shape_dir = os.path.join(output_dir, shape)\n",
    "    files = os.listdir(shape_dir)\n",
    "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    os.makedirs(os.path.join(train_dir, shape), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, shape), exist_ok=True)\n",
    "    \n",
    "    for f in train_files:\n",
    "        shutil.move(os.path.join(shape_dir, f), os.path.join(train_dir, shape, f))\n",
    "    for f in test_files:\n",
    "        shutil.move(os.path.join(shape_dir, f), os.path.join(test_dir, shape, f))\n",
    "\n",
    "shutil.rmtree(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "089b3324-9ecc-4d13-9f05-cb85aa9e7f2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Datos con ruido y rotaciones\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Entrenar modelos y evaluar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d1ad227-46ae-4825-9e72-c8cda07e70fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# Configuración del espacio de simetrías\n",
    "gspace = gspaces.Rot2dOnR2(N=4)\n",
    "\n",
    "# Definición de la G-CNN\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        \n",
    "        self.input_type = enn.FieldType(gspace, [gspace.trivial_repr])\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "762f79e1-1d2f-4f89-bd51-4fa3feeb966d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2558\n",
      "Epoch [2/10], Loss: 0.1347\n",
      "Epoch [3/10], Loss: 0.0198\n",
      "Epoch [4/10], Loss: 0.0065\n",
      "Epoch [5/10], Loss: 0.0040\n",
      "Epoch [6/10], Loss: 0.0028\n",
      "Epoch [7/10], Loss: 0.0022\n",
      "Epoch [8/10], Loss: 0.0017\n",
      "Epoch [9/10], Loss: 0.0014\n",
      "Epoch [10/10], Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Configuración y entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c377ef51-93ba-480a-b01c-9c28cfd73213",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GCNN: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of GCNN: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58434e0-7e86-4292-8a87-6b84a770fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47029b6d-fa0e-49a5-bb54-a1345c9be60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6cafc14-93d4-4f5a-8ec9-8cf2d8d5155e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ce6bf52-d11e-4104-8f92-e3366aacd091",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9011\n",
      "Epoch [2/10], Loss: 0.0942\n",
      "Epoch [3/10], Loss: 0.0160\n",
      "Epoch [4/10], Loss: 0.0049\n",
      "Epoch [5/10], Loss: 0.0026\n",
      "Epoch [6/10], Loss: 0.0020\n",
      "Epoch [7/10], Loss: 0.0017\n",
      "Epoch [8/10], Loss: 0.0014\n",
      "Epoch [9/10], Loss: 0.0011\n",
      "Epoch [10/10], Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Configuración y entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f92853c8-62e4-4551-a226-0da60b38fdb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of CNN: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf1091-be93-4c39-bf28-8537d1748422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bd2cd43-8e8f-4a6c-8cc1-ee7164390758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Training CNN...\n",
      "Epoch [1/10], Loss: 1.3597\n",
      "Epoch [2/10], Loss: 1.0233\n",
      "Epoch [3/10], Loss: 0.8902\n",
      "Epoch [4/10], Loss: 0.8059\n",
      "Epoch [5/10], Loss: 0.7392\n",
      "Epoch [6/10], Loss: 0.6799\n",
      "Epoch [7/10], Loss: 0.6282\n",
      "Epoch [8/10], Loss: 0.5853\n",
      "Epoch [9/10], Loss: 0.5427\n",
      "Epoch [10/10], Loss: 0.5034\n",
      "Evaluating CNN...\n",
      "Accuracy: 66.33%\n",
      "\n",
      "Training G-CNN...\n",
      "Epoch [1/10], Loss: 1.4140\n",
      "Epoch [2/10], Loss: 1.0540\n",
      "Epoch [3/10], Loss: 0.9062\n",
      "Epoch [4/10], Loss: 0.8018\n",
      "Epoch [5/10], Loss: 0.7127\n",
      "Epoch [6/10], Loss: 0.6321\n",
      "Epoch [7/10], Loss: 0.5666\n",
      "Epoch [8/10], Loss: 0.5100\n",
      "Epoch [9/10], Loss: 0.4501\n",
      "Epoch [10/10], Loss: 0.4084\n",
      "Evaluating G-CNN...\n",
      "Accuracy: 64.45%\n",
      "\n",
      "Final Results: CNN Accuracy = 66.33%, G-CNN Accuracy = 64.45%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "\n",
    "# 1. Configuración de datos CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Redimensionar para las G-CNN\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. Configuración de la G-CNN\n",
    "gspace = gspaces.Rot2dOnR2(N=8)  # Rotaciones discretas C8 (0°, 45°, 90°, ..., 315°)\n",
    "\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNN, self).__init__()\n",
    "        # Entrada con 3 canales (RGB)\n",
    "        self.input_type = enn.FieldType(gspace, 3 * [gspace.trivial_repr])  # 3 canales con representación trivial\n",
    "        self.conv1 = enn.R2Conv(\n",
    "            in_type=self.input_type,\n",
    "            out_type=enn.FieldType(gspace, 16 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = enn.ReLU(self.conv1.out_type)\n",
    "        self.conv2 = enn.R2Conv(\n",
    "            in_type=self.conv1.out_type,\n",
    "            out_type=enn.FieldType(gspace, 32 * [gspace.regular_repr]),\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = enn.ReLU(self.conv2.out_type)\n",
    "        self.pool = enn.PointwiseMaxPool(self.conv2.out_type, kernel_size=2, stride=2)\n",
    "        self.gpool = enn.GroupPooling(self.pool.out_type)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, 10)  # 10 clases de CIFAR-10\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convertir a GeometricTensor\n",
    "        x = enn.GeometricTensor(x, self.input_type)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.gpool(x).tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. Configuración de una CNN estándar\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 4. Entrenamiento y evaluación\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# 5. Comparación entre CNN estándar y G-CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CNN estándar\n",
    "cnn = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "print(\"\\nTraining CNN...\")\n",
    "train_model(cnn, train_loader, criterion, optimizer, num_epochs=10)\n",
    "print(\"Evaluating CNN...\")\n",
    "cnn_accuracy = evaluate_model(cnn, test_loader)\n",
    "\n",
    "# G-CNN\n",
    "gcnn = GCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcnn.parameters(), lr=0.001)\n",
    "print(\"\\nTraining G-CNN...\")\n",
    "train_model(gcnn, train_loader, criterion, optimizer, num_epochs=10)\n",
    "print(\"Evaluating G-CNN...\")\n",
    "gcnn_accuracy = evaluate_model(gcnn, test_loader)\n",
    "\n",
    "# Comparación\n",
    "print(f\"\\nFinal Results: CNN Accuracy = {cnn_accuracy:.2f}%, G-CNN Accuracy = {gcnn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b71a0-183a-4df6-b80d-add267573a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda5f53-c416-4d8b-99c9-b5412f409919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap",
   "language": "python",
   "name": "umap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
