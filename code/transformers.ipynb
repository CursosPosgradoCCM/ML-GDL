{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e01e0e3-0ffd-4167-882c-51db83371a66",
   "metadata": {},
   "source": [
    "<h1>Ejemplos de Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5a670-2ed8-467c-947c-b4e8685f2e22",
   "metadata": {},
   "source": [
    "- Como vimos, transformers es útil para el procesamiento de palabras. 🤗 Hugging Face es una empresa y una comunidad muy influyente en el campo del procesamiento de lenguaje natural (NLP) y el aprendizaje automático. Su misión principal es hacer que la tecnología de aprendizaje profundo (Deep Learning) sea accesible y útil para todos, proporcionando herramientas y modelos preentrenados que simplifican el desarrollo de aplicaciones de inteligencia artificial.\n",
    "\n",
    "- La principal contribución de Hugging Face a la comunidad de Machine Learning es su librería Transformers. Esta librería proporciona una interfaz fácil de usar para trabajar con modelos Transformer preentrenados, como BERT, GPT, T5, DistilBERT, y muchos otros, que están diseñados para tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "- Con Transformers, es posible cargar modelos preentrenados y utilizarlos para una variedad de tareas, como:\n",
    "  * Clasificación de texto.\n",
    "  * Traducción de idiomas.\n",
    "  * Generación de texto.\n",
    "  * Resumido de texto.\n",
    "  * Análisis de sentimientos.\n",
    "  * Pregunta y respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5492955-a04a-44be-aa14-011d4c883282",
   "metadata": {},
   "source": [
    "<h2>Instalamos transformers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838f72c9-fa40-4ec7-b4f2-abfa370bd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7538a292-dfd6-42cc-8227-633435dcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b24ee1-133f-424a-aedc-93aa5dbf30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install tqdm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365e69d-8251-4ca5-b23a-21f6d93fd2c8",
   "metadata": {},
   "source": [
    "<h2>Ejemplo de clasificación de texto y análisis de sentimientos.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ab538-ad76-424e-a19c-00f3326178d6",
   "metadata": {},
   "source": [
    "<h3>Importamos modelo y tokenizer. Elegimos que se predecirá</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50af543-4eec-4dc1-a69e-30fc8fa4c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification #Importa tokenizer y modelo pre-entrenado\n",
    "#Este modelo trata de clasificar el texto de acuerdo a si una oración en inglés es considerada como un sentimiento positivo o negativo\n",
    "\n",
    "#Se genera el tokenizer y el modelp\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01288ced-95f5-4736-a99a-f3798144c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se agrega un elemento a predecir. return_tensors sirve para especificar el tipo de tensor que va a rgresar, en este caso un tensor de pytorch\n",
    "inputs = tokenizer(\"I am so happy right now\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ba85f5c-56dd-48f8-85aa-58c948b529d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 17662,  2227,  2124,  2005,  1029,   102, 17662,\n",
       "          2227,  2003,  1037,  2194,  2008,  3640,  2019,  2330,  1011,  3120,\n",
       "          4132,  2005,  3019,  2653,  6364,  1006, 17953,  2361,  1007,  8518,\n",
       "          1012,  2037,  4132,  4107,  3653,  1011,  4738,  4275,  1010,  2951,\n",
       "         13462,  2015,  1010,  1998,  5906,  2000,  2191,  2009,  6082,  2005,\n",
       "          9797,  2000,  3857, 17953,  2361,  5097,  1012,  2027,  2024,  2092,\n",
       "          2124,  2005,  2037, 10938,  2121,  4275,  2066, 14324,  1010, 14246,\n",
       "          2102,  1011,  1016,  1010,  1056,  2629,  1010,  1998,  2500,  1010,\n",
       "          2029,  2031,  2042,  4235,  4233,  1999,  1996,  9932,  2451,  1012,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72103577-fef4-4992-9cd8-e5780fb95ca4",
   "metadata": {},
   "source": [
    "<h3>Predecimos y vemos la salida</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe05c8ee-94cb-4b64-8e50-b7a8df722094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3311,  4.6930]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Al momento de ingresar el input automaticamente se vuelve a calcular el gradiente, pero con la siguiente instrucción evita esto para mejorar\n",
    "#rendimiento\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs)##se pasa **inputs pues es una manera de pasar de un diccionario su llave y objeto a las funciones\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81362428-5afc-4912-a270-c479885bc73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits #elegimos .logits seleccionar los puntajes, en la primer posicion del tensor es puntaje para negativos\n",
    "                                    # y la segunda para positivos\n",
    "predicted_class_id = logits.argmax().item() #Se selleciona la clase con mayor puntaje\n",
    "model.config.id2label[predicted_class_id] #Muestra la clase predicha por medio de un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94874410-5f2c-44ae-a616-f89f0e53fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86991c93-e4e3-4805-ac93-07f77ff70f3b",
   "metadata": {},
   "source": [
    "<h3>Existen distintos modelos, por ejemplo hay en español</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a8df0a0-49a2-487c-ac97-13d7a69e1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El sentimiento del texto es: neutral\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Cargar el modelo y el tokenizer multilingüe para clasificación de sentimientos\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Texto en español\n",
    "texto = \":c \"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(texto, return_tensors=\"pt\")\n",
    "\n",
    "# Realizar inferencia sin calcular gradientes\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Obtener la predicción de la clase\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "# Las etiquetas de sentimiento en este modelo son de 0 (muy negativo) a 4 (muy positivo)\n",
    "etiquetas = [\"muy negativo\", \"negativo\", \"neutral\", \"positivo\", \"muy positivo\"]\n",
    "print(f\"El sentimiento del texto es: {etiquetas[predicted_class_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcc0f2-36b7-4f2e-a311-eea2adce1b8f",
   "metadata": {},
   "source": [
    "<h2>Es posible importar la arquitectura deseada, por ejemplo, de clasificación y entrenar con datos propios</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a870ad0-fe51-4895-88e9-bc21b387f34f",
   "metadata": {},
   "source": [
    "<h3>Usaremos la base de datos de IMDb</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4324db87-621b-4c02-a203-2f5852f2b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef35ff69-4111-445d-8165-e0a4991e6a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e1c209-cf34-4243-915f-0e2dba125e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6472324b-fb34-497d-846b-c34b11f039c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d033d0a-2638-474c-9ec0-4cb5f92ee84f",
   "metadata": {},
   "source": [
    "<h3>Importamos los modulos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54983e8f-4ffb-4241-8130-fb402c47408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbc001-c415-4a66-9f9d-af0858a5faf0",
   "metadata": {},
   "source": [
    "<h3>Cargamos la base de datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8964f5-91aa-456e-b53a-9075ddc820ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec978206-45fd-4030-9f0d-e2a4dbbdac5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This film is terrible. You don't really need to read this review further. If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).<br /><br />The acting is horrendous... serious amateur hour. Throughout the movie I thought that it was interesting that they found someone who speaks and looks like Michael Madsen, only to find out that it is actually him! A new low even for him!!<br /><br />The plot is terrible. People who claim that it is original or good have probably never seen a decent movie before. Even by the standard of Hollywood action flicks, this is a terrible movie.<br /><br />Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a46fc-2647-4f3b-9915-783ad6769bd8",
   "metadata": {},
   "source": [
    "<h3>Iniciamos el tokenizer y lo aplicamos a los datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a9610ec-5db3-4ac4-bcfa-e5c655a1bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24bb78d6-5f63-4784-a46b-bb198bd4d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y validación (si no existe una división)\n",
    "train_dataset = tokenized_datasets['train']\n",
    "val_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aab85-f574-4565-b018-4c211024daed",
   "metadata": {},
   "source": [
    "<h3>Inicializamos el modelo con los parámetros deseados</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79cd837-aca7-4017-bb60-38eceb1804fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter-user4/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Configuración del modelo y argumentos de entrenamiento\n",
    "imdb_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio donde se guardarán los resultados\n",
    "    evaluation_strategy=\"epoch\",     # Evaluar por época\n",
    "    learning_rate=2e-5,              # Tasa de aprendizaje\n",
    "    per_device_train_batch_size=8,   # Tamaño del batch\n",
    "    per_device_eval_batch_size=8,    # Tamaño del batch para evaluación\n",
    "    num_train_epochs=3,              # Número de épocas\n",
    "    weight_decay=0.01,               # Decaimiento de peso\n",
    ")\n",
    "\n",
    "# Función de métricas (precisión)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = predictions.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c9366-4683-46de-b16e-b77f58137a5d",
   "metadata": {},
   "source": [
    "<h3>Entrenamos, esto puede tardar cerca de una hora</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "242dea18-9c4e-453f-85b6-dcf6c2b4ffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:15:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.258317</td>\n",
       "      <td>0.920880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.937120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.325210</td>\n",
       "      <td>0.939040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.17759403198242188, metrics={'train_runtime': 4535.0153, 'train_samples_per_second': 16.538, 'train_steps_per_second': 2.067, 'total_flos': 1.9733329152e+16, 'train_loss': 0.17759403198242188, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Entrenador\n",
    "trainer = Trainer(\n",
    "    model=imdb_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4206fc-4f77-4e7d-905d-db2e9715c6bc",
   "metadata": {},
   "source": [
    "<h3>Evaluamos los resultados con los datos de test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7af6ccc-0a95-4f04-a2bd-4fcb54cf538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 06:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32521024346351624, 'eval_accuracy': 0.93904, 'eval_runtime': 383.164, 'eval_samples_per_second': 65.246, 'eval_steps_per_second': 8.156, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d102d5-ced6-4f72-8d68-af12ac1b5ac9",
   "metadata": {},
   "source": [
    "<h3>Realizamos predicciones</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81335e37-9992-4fda-82d2-c4346eca9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Verifica si tienes una GPU disponible, esto pues parece que el modelo y el input se pueden encontrar en lugares diferentes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "imdb_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "imdb_model.to(device)\n",
    "\n",
    "# Define el texto para predecir\n",
    "text = \"I love this movie\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Mover los tensores de entrada al mismo dispositivo que el modelo\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Realizar la predicción\n",
    "with torch.no_grad():  # No se necesita gradiente para la predicción\n",
    "    outputs = imdb_model(**inputs)\n",
    "\n",
    "# Obtener la predicción\n",
    "prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Imprimir la predicción\n",
    "print(f\"Predicción: {prediction.item()}\")  # 0 = negativo, 1 = positivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b843de-384b-45d5-887e-326bd01011d6",
   "metadata": {},
   "source": [
    "<h2>Traducción de idiomas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34731ed9-68c7-487a-9f0c-69ebdf2e7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: --no-displayr\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-displayr\n",
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ca3f687-7b11-4a34-b443-c6ee3fcc6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fefde924-b666-4da1-800c-9fa263316d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: i live in that house\n",
      "Texto traducido: Vivo en esa casa.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para traducción inglés -> español\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"  # Traducción de inglés a español\n",
    "translate_model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto en inglés que se quiere traducir\n",
    "text = \"i live in that house\"\n",
    "\n",
    "# Tokenizar el texto de entrada\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Realizar la traducción\n",
    "translated = translate_model.generate(**inputs)\n",
    "\n",
    "# Decodificar la traducción de vuelta a texto\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el texto traducido\n",
    "print(f\"Texto original: {text}\")\n",
    "print(f\"Texto traducido: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2c533-8419-45dc-b3df-a17cd1dc5a03",
   "metadata": {},
   "source": [
    "<h2>Generar texto</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "698b222a-f0b6-43fb-95fc-f6e478206a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today its wendsday an tomorrow's a-day, and the way it's structured, it is the first time it has done so.\"\n",
      "\n",
      "The president is expected to make the announcement on Monday, but no date has been set.\n",
      ".@JPMorgan will be announcing this morning. — Robert Costa (@costareports) October 30, 2015\n",
      ", \"It's been two days since the Trump administration announced a new program to train and equip American military and intelligence personnel in the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo preentrenado y el tokenizador\n",
    "model_name = \"gpt2\"  # Usamos el modelo base de GPT-2\n",
    "generative_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Asegurarse de que el modelo esté en el dispositivo adecuado (CPU o GPU)\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generative_model.to(device)\n",
    "\n",
    "# Texto de entrada (prompt)\n",
    "prompt = \"Today its wendsday an tomorrow\"\n",
    "\n",
    "# Tokenizar el prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto a partir del prompt\n",
    "output = generative_model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,       # Longitud máxima de la secuencia generada\n",
    "    num_return_sequences=1,  # Número de secuencias a generar\n",
    "    no_repeat_ngram_size=2,  # Evita la repetición de n-grams\n",
    "    temperature=0.7,      # Controla la aleatoriedad del modelo\n",
    "    top_k=50,             # Limita a las top k probabilidades\n",
    "    top_p=0.95,           # Usar muestreo nuclear (probabilidades acumuladas)\n",
    "    do_sample=True        # Usar muestreo en lugar de decodificación codiciosa\n",
    ")\n",
    "\n",
    "# Decodificar la salida generada\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el texto generado\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286e0cb-004f-404c-a719-456c94ddce8f",
   "metadata": {},
   "source": [
    "<h2>Resumir texto</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bc4aa4b-3590-42da-a5de-e159007dd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen generado: Bioinformatics is a subdiscipline of genetics and genomics. It involves using computer technology to collect, store, analyze and disseminate biological data and information. It can include DNA and amino acid sequences or annotations about those sequences.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para resumen\n",
    "model_name = \"facebook/bart-large-cnn\"  # Modelo de BART preentrenado para resumen\n",
    "resum_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto que se desea resumir\n",
    "text = \"\"\"\n",
    "Bioinformatics, as related to genetics and genomics, is a scientific subdiscipline that involves using computer technology to collect, store, analyze and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar el texto de entrada\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generar el resumen\n",
    "summary_ids = resum_model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=150,  # Longitud máxima del resumen\n",
    "    min_length=50,   # Longitud mínima del resumen\n",
    "    length_penalty=2.0,  # Penaliza resúmenes demasiado cortos\n",
    "    num_beams=4,     # Número de \"haz\" para la búsqueda de secuencias\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decodificar el resumen generado\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el resumen\n",
    "print(f\"Resumen generado: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51334b-a182-46be-aab2-7861d4859049",
   "metadata": {},
   "source": [
    "<h2>Responder preguntas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "599cb377-b7ba-41e4-a797-69ebd42dd9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: What is Hugging Face known for?\n",
      "Respuesta: their transformer models\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para preguntas y respuestas\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "respond_model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto que se utiliza como base para responder preguntas\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that provides an open-source platform for natural language processing (NLP) tasks. \n",
    "Their platform offers pre-trained models, datasets, and tools to make it easier for developers to build NLP applications.\n",
    "They are well known for their transformer models like BERT, GPT-2, T5, and others, which have been widely adopted in the AI community.\n",
    "\"\"\"\n",
    "\n",
    "# Pregunta que se quiere responder\n",
    "question = \"What is Hugging Face known for?\"\n",
    "\n",
    "# Tokenizar el texto de entrada (contexto y pregunta)\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Obtener los tensores de entrada\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Realizar la predicción (responder la pregunta)\n",
    "outputs = respond_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Obtener los índices de las respuestas (start y end)\n",
    "start_index = torch.argmax(outputs.start_logits)\n",
    "end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "# Convertir los índices de respuesta a texto\n",
    "answer_ids = input_ids[0][start_index:end_index + 1]\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap",
   "language": "python",
   "name": "umap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
