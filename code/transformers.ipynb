{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e01e0e3-0ffd-4167-882c-51db83371a66",
   "metadata": {},
   "source": [
    "<h1>Ejemplos de Transformers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5a670-2ed8-467c-947c-b4e8685f2e22",
   "metadata": {},
   "source": [
    "- Como vimos, transformers es √∫til para el procesamiento de palabras. ü§ó Hugging Face es una empresa y una comunidad muy influyente en el campo del procesamiento de lenguaje natural (NLP) y el aprendizaje autom√°tico. Su misi√≥n principal es hacer que la tecnolog√≠a de aprendizaje profundo (Deep Learning) sea accesible y √∫til para todos, proporcionando herramientas y modelos preentrenados que simplifican el desarrollo de aplicaciones de inteligencia artificial.\n",
    "\n",
    "- La principal contribuci√≥n de Hugging Face a la comunidad de Machine Learning es su librer√≠a Transformers. Esta librer√≠a proporciona una interfaz f√°cil de usar para trabajar con modelos Transformer preentrenados, como BERT, GPT, T5, DistilBERT, y muchos otros, que est√°n dise√±ados para tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "- Con Transformers, es posible cargar modelos preentrenados y utilizarlos para una variedad de tareas, como:\n",
    "  * Clasificaci√≥n de texto.\n",
    "  * Traducci√≥n de idiomas.\n",
    "  * Generaci√≥n de texto.\n",
    "  * Resumido de texto.\n",
    "  * An√°lisis de sentimientos.\n",
    "  * Pregunta y respuesta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5492955-a04a-44be-aa14-011d4c883282",
   "metadata": {},
   "source": [
    "<h2>Instalamos transformers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838f72c9-fa40-4ec7-b4f2-abfa370bd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7538a292-dfd6-42cc-8227-633435dcf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b24ee1-133f-424a-aedc-93aa5dbf30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "pip install tqdm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365e69d-8251-4ca5-b23a-21f6d93fd2c8",
   "metadata": {},
   "source": [
    "<h2>Ejemplo de clasificaci√≥n de texto y an√°lisis de sentimientos.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ab538-ad76-424e-a19c-00f3326178d6",
   "metadata": {},
   "source": [
    "<h3>Importamos modelo y tokenizer. Elegimos que se predecir√°</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50af543-4eec-4dc1-a69e-30fc8fa4c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification #Importa tokenizer y modelo pre-entrenado\n",
    "#Este modelo trata de clasificar el texto de acuerdo a si una oraci√≥n en ingl√©s es considerada como un sentimiento positivo o negativo\n",
    "\n",
    "#Se genera el tokenizer y el modelp\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01288ced-95f5-4736-a99a-f3798144c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se agrega un elemento a predecir. return_tensors sirve para especificar el tipo de tensor que va a rgresar, en este caso un tensor de pytorch\n",
    "inputs = tokenizer(\"I am so happy right now\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ba85f5c-56dd-48f8-85aa-58c948b529d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003, 17662,  2227,  2124,  2005,  1029,   102, 17662,\n",
       "          2227,  2003,  1037,  2194,  2008,  3640,  2019,  2330,  1011,  3120,\n",
       "          4132,  2005,  3019,  2653,  6364,  1006, 17953,  2361,  1007,  8518,\n",
       "          1012,  2037,  4132,  4107,  3653,  1011,  4738,  4275,  1010,  2951,\n",
       "         13462,  2015,  1010,  1998,  5906,  2000,  2191,  2009,  6082,  2005,\n",
       "          9797,  2000,  3857, 17953,  2361,  5097,  1012,  2027,  2024,  2092,\n",
       "          2124,  2005,  2037, 10938,  2121,  4275,  2066, 14324,  1010, 14246,\n",
       "          2102,  1011,  1016,  1010,  1056,  2629,  1010,  1998,  2500,  1010,\n",
       "          2029,  2031,  2042,  4235,  4233,  1999,  1996,  9932,  2451,  1012,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72103577-fef4-4992-9cd8-e5780fb95ca4",
   "metadata": {},
   "source": [
    "<h3>Predecimos y vemos la salida</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe05c8ee-94cb-4b64-8e50-b7a8df722094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3311,  4.6930]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Al momento de ingresar el input automaticamente se vuelve a calcular el gradiente, pero con la siguiente instrucci√≥n evita esto para mejorar\n",
    "#rendimiento\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs)##se pasa **inputs pues es una manera de pasar de un diccionario su llave y objeto a las funciones\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81362428-5afc-4912-a270-c479885bc73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits #elegimos .logits seleccionar los puntajes, en la primer posicion del tensor es puntaje para negativos\n",
    "                                    # y la segunda para positivos\n",
    "predicted_class_id = logits.argmax().item() #Se selleciona la clase con mayor puntaje\n",
    "model.config.id2label[predicted_class_id] #Muestra la clase predicha por medio de un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94874410-5f2c-44ae-a616-f89f0e53fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86991c93-e4e3-4805-ac93-07f77ff70f3b",
   "metadata": {},
   "source": [
    "<h3>Existen distintos modelos, por ejemplo hay en espa√±ol</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a8df0a0-49a2-487c-ac97-13d7a69e1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El sentimiento del texto es: neutral\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Cargar el modelo y el tokenizer multiling√ºe para clasificaci√≥n de sentimientos\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Texto en espa√±ol\n",
    "texto = \":c \"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(texto, return_tensors=\"pt\")\n",
    "\n",
    "# Realizar inferencia sin calcular gradientes\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Obtener la predicci√≥n de la clase\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "# Las etiquetas de sentimiento en este modelo son de 0 (muy negativo) a 4 (muy positivo)\n",
    "etiquetas = [\"muy negativo\", \"negativo\", \"neutral\", \"positivo\", \"muy positivo\"]\n",
    "print(f\"El sentimiento del texto es: {etiquetas[predicted_class_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcc0f2-36b7-4f2e-a311-eea2adce1b8f",
   "metadata": {},
   "source": [
    "<h2>Es posible importar la arquitectura deseada, por ejemplo, de clasificaci√≥n y entrenar con datos propios</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a870ad0-fe51-4895-88e9-bc21b387f34f",
   "metadata": {},
   "source": [
    "<h3>Usaremos la base de datos de IMDb</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4324db87-621b-4c02-a203-2f5852f2b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef35ff69-4111-445d-8165-e0a4991e6a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3e1c209-cf34-4243-915f-0e2dba125e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6472324b-fb34-497d-846b-c34b11f039c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d033d0a-2638-474c-9ec0-4cb5f92ee84f",
   "metadata": {},
   "source": [
    "<h3>Importamos los modulos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54983e8f-4ffb-4241-8130-fb402c47408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbc001-c415-4a66-9f9d-af0858a5faf0",
   "metadata": {},
   "source": [
    "<h3>Cargamos la base de datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8964f5-91aa-456e-b53a-9075ddc820ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec978206-45fd-4030-9f0d-e2a4dbbdac5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This film is terrible. You don't really need to read this review further. If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).<br /><br />The acting is horrendous... serious amateur hour. Throughout the movie I thought that it was interesting that they found someone who speaks and looks like Michael Madsen, only to find out that it is actually him! A new low even for him!!<br /><br />The plot is terrible. People who claim that it is original or good have probably never seen a decent movie before. Even by the standard of Hollywood action flicks, this is a terrible movie.<br /><br />Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a46fc-2647-4f3b-9915-783ad6769bd8",
   "metadata": {},
   "source": [
    "<h3>Iniciamos el tokenizer y lo aplicamos a los datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a9610ec-5db3-4ac4-bcfa-e5c655a1bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaci√≥n\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24bb78d6-5f63-4784-a46b-bb198bd4d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y validaci√≥n (si no existe una divisi√≥n)\n",
    "train_dataset = tokenized_datasets['train']\n",
    "val_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aab85-f574-4565-b018-4c211024daed",
   "metadata": {},
   "source": [
    "<h3>Inicializamos el modelo con los par√°metros deseados</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79cd837-aca7-4017-bb60-38eceb1804fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter-user4/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del modelo y argumentos de entrenamiento\n",
    "imdb_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio donde se guardar√°n los resultados\n",
    "    evaluation_strategy=\"epoch\",     # Evaluar por √©poca\n",
    "    learning_rate=2e-5,              # Tasa de aprendizaje\n",
    "    per_device_train_batch_size=8,   # Tama√±o del batch\n",
    "    per_device_eval_batch_size=8,    # Tama√±o del batch para evaluaci√≥n\n",
    "    num_train_epochs=3,              # N√∫mero de √©pocas\n",
    "    weight_decay=0.01,               # Decaimiento de peso\n",
    ")\n",
    "\n",
    "# Funci√≥n de m√©tricas (precisi√≥n)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = predictions.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c9366-4683-46de-b16e-b77f58137a5d",
   "metadata": {},
   "source": [
    "<h3>Entrenamos, esto puede tardar cerca de una hora</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "242dea18-9c4e-453f-85b6-dcf6c2b4ffd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:15:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.258317</td>\n",
       "      <td>0.920880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.263804</td>\n",
       "      <td>0.937120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.325210</td>\n",
       "      <td>0.939040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.17759403198242188, metrics={'train_runtime': 4535.0153, 'train_samples_per_second': 16.538, 'train_steps_per_second': 2.067, 'total_flos': 1.9733329152e+16, 'train_loss': 0.17759403198242188, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Entrenador\n",
    "trainer = Trainer(\n",
    "    model=imdb_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4206fc-4f77-4e7d-905d-db2e9715c6bc",
   "metadata": {},
   "source": [
    "<h3>Evaluamos los resultados con los datos de test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7af6ccc-0a95-4f04-a2bd-4fcb54cf538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 06:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32521024346351624, 'eval_accuracy': 0.93904, 'eval_runtime': 383.164, 'eval_samples_per_second': 65.246, 'eval_steps_per_second': 8.156, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d102d5-ced6-4f72-8d68-af12ac1b5ac9",
   "metadata": {},
   "source": [
    "<h3>Realizamos predicciones</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81335e37-9992-4fda-82d2-c4346eca9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicci√≥n: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Verifica si tienes una GPU disponible, esto pues parece que el modelo y el input se pueden encontrar en lugares diferentes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "imdb_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "imdb_model.to(device)\n",
    "\n",
    "# Define el texto para predecir\n",
    "text = \"I love this movie\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Mover los tensores de entrada al mismo dispositivo que el modelo\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Realizar la predicci√≥n\n",
    "with torch.no_grad():  # No se necesita gradiente para la predicci√≥n\n",
    "    outputs = imdb_model(**inputs)\n",
    "\n",
    "# Obtener la predicci√≥n\n",
    "prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Imprimir la predicci√≥n\n",
    "print(f\"Predicci√≥n: {prediction.item()}\")  # 0 = negativo, 1 = positivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b843de-384b-45d5-887e-326bd01011d6",
   "metadata": {},
   "source": [
    "<h2>Traducci√≥n de idiomas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34731ed9-68c7-487a-9f0c-69ebdf2e7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: --no-displayr\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-displayr\n",
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ca3f687-7b11-4a34-b443-c6ee3fcc6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fefde924-b666-4da1-800c-9fa263316d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: i live in that house\n",
      "Texto traducido: Vivo en esa casa.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para traducci√≥n ingl√©s -> espa√±ol\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"  # Traducci√≥n de ingl√©s a espa√±ol\n",
    "translate_model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto en ingl√©s que se quiere traducir\n",
    "text = \"i live in that house\"\n",
    "\n",
    "# Tokenizar el texto de entrada\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Realizar la traducci√≥n\n",
    "translated = translate_model.generate(**inputs)\n",
    "\n",
    "# Decodificar la traducci√≥n de vuelta a texto\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el texto traducido\n",
    "print(f\"Texto original: {text}\")\n",
    "print(f\"Texto traducido: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2c533-8419-45dc-b3df-a17cd1dc5a03",
   "metadata": {},
   "source": [
    "<h2>Generar texto</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "698b222a-f0b6-43fb-95fc-f6e478206a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today its wendsday an tomorrow's a-day, and the way it's structured, it is the first time it has done so.\"\n",
      "\n",
      "The president is expected to make the announcement on Monday, but no date has been set.\n",
      ".@JPMorgan will be announcing this morning. ‚Äî Robert Costa (@costareports) October 30, 2015\n",
      ", \"It's been two days since the Trump administration announced a new program to train and equip American military and intelligence personnel in the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Cargar el modelo preentrenado y el tokenizador\n",
    "model_name = \"gpt2\"  # Usamos el modelo base de GPT-2\n",
    "generative_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Asegurarse de que el modelo est√© en el dispositivo adecuado (CPU o GPU)\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generative_model.to(device)\n",
    "\n",
    "# Texto de entrada (prompt)\n",
    "prompt = \"Today its wendsday an tomorrow\"\n",
    "\n",
    "# Tokenizar el prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar texto a partir del prompt\n",
    "output = generative_model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,       # Longitud m√°xima de la secuencia generada\n",
    "    num_return_sequences=1,  # N√∫mero de secuencias a generar\n",
    "    no_repeat_ngram_size=2,  # Evita la repetici√≥n de n-grams\n",
    "    temperature=0.7,      # Controla la aleatoriedad del modelo\n",
    "    top_k=50,             # Limita a las top k probabilidades\n",
    "    top_p=0.95,           # Usar muestreo nuclear (probabilidades acumuladas)\n",
    "    do_sample=True        # Usar muestreo en lugar de decodificaci√≥n codiciosa\n",
    ")\n",
    "\n",
    "# Decodificar la salida generada\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el texto generado\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286e0cb-004f-404c-a719-456c94ddce8f",
   "metadata": {},
   "source": [
    "<h2>Resumir texto</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bc4aa4b-3590-42da-a5de-e159007dd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen generado: Bioinformatics is a subdiscipline of genetics and genomics. It involves using computer technology to collect, store, analyze and disseminate biological data and information. It can include DNA and amino acid sequences or annotations about those sequences.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para resumen\n",
    "model_name = \"facebook/bart-large-cnn\"  # Modelo de BART preentrenado para resumen\n",
    "resum_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto que se desea resumir\n",
    "text = \"\"\"\n",
    "Bioinformatics, as related to genetics and genomics, is a scientific subdiscipline that involves using computer technology to collect, store, analyze and disseminate biological data and information, such as DNA and amino acid sequences or annotations about those sequences.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizar el texto de entrada\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generar el resumen\n",
    "summary_ids = resum_model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=150,  # Longitud m√°xima del resumen\n",
    "    min_length=50,   # Longitud m√≠nima del resumen\n",
    "    length_penalty=2.0,  # Penaliza res√∫menes demasiado cortos\n",
    "    num_beams=4,     # N√∫mero de \"haz\" para la b√∫squeda de secuencias\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decodificar el resumen generado\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Mostrar el resumen\n",
    "print(f\"Resumen generado: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51334b-a182-46be-aab2-7861d4859049",
   "metadata": {},
   "source": [
    "<h2>Responder preguntas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "599cb377-b7ba-41e4-a797-69ebd42dd9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: What is Hugging Face known for?\n",
      "Respuesta: their transformer models\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador preentrenado para preguntas y respuestas\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "respond_model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Texto que se utiliza como base para responder preguntas\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that provides an open-source platform for natural language processing (NLP) tasks. \n",
    "Their platform offers pre-trained models, datasets, and tools to make it easier for developers to build NLP applications.\n",
    "They are well known for their transformer models like BERT, GPT-2, T5, and others, which have been widely adopted in the AI community.\n",
    "\"\"\"\n",
    "\n",
    "# Pregunta que se quiere responder\n",
    "question = \"What is Hugging Face known for?\"\n",
    "\n",
    "# Tokenizar el texto de entrada (contexto y pregunta)\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Obtener los tensores de entrada\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Realizar la predicci√≥n (responder la pregunta)\n",
    "outputs = respond_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Obtener los √≠ndices de las respuestas (start y end)\n",
    "start_index = torch.argmax(outputs.start_logits)\n",
    "end_index = torch.argmax(outputs.end_logits)\n",
    "\n",
    "# Convertir los √≠ndices de respuesta a texto\n",
    "answer_ids = input_ids[0][start_index:end_index + 1]\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap",
   "language": "python",
   "name": "umap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
