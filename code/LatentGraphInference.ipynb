{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abc9a81-e81d-4d58-937c-de9a43c8890f",
   "metadata": {},
   "source": [
    "# __Latent Graph Inference__\n",
    "\n",
    "Es el proceso de aprender o deducir la estructura de un grafo implícito a partir de datos en los que las relaciones explícitas entre las entidades (nodos) no están directamente disponibles. En este contexto, un grafo latente se refiere a una representación de las relaciones subyacentes que se infieren durante el análisis o entrenamiento de un modelo, en lugar de ser proporcionadas como entrada.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceee1ef-e32b-4708-a067-f0e63065524a",
   "metadata": {},
   "source": [
    "## **Conceptos Básicos**\n",
    "1. Nodos: Representan entidades o elementos en los datos (por ejemplo, personas, palabras, genes).\n",
    "2. Aristas (Edges): Representan las relaciones entre nodos. ¡Pero! **En un grafo latente, estas relaciones no están explícitas, sino que se infieren a partir de patrones en los datos**.\n",
    "3. Matriz de Adyacencia:\n",
    "    * En un grafo explícito, es una matriz $A$ donde $A_{ij} = 1$ indica una conexión entre los nodos $i$ y $j$.\n",
    "    * **En la inferencia de grafos latentes, esta matriz es aprendida durante el entrenamiento del modelo.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255352bf-c56d-4540-ac35-8d4595de42d4",
   "metadata": {},
   "source": [
    "## **Objetivo**\n",
    "\n",
    "El objetivo de la inferencia de grafos latentes es **descubrir relaciones implícitas** que pueden proporcionar *información adicional* para tareas de aprendizaje como clasificación, predicción o agrupamiento.\n",
    "\n",
    "Por ejemplo:\n",
    "* En redes sociales, infiere relaciones ocultas basadas en intereses comunes o comportamientos similares.\n",
    "* En biología, identifica interacciones no observadas entre proteínas o genes.\n",
    "* En NLP, extrae dependencias implícitas entre palabras o conceptos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff9f18-0cbc-474f-a124-c49194611afe",
   "metadata": {},
   "source": [
    "## **Métodos Usados para Inferir Grafos**\n",
    "1. **Graph Neural Networks (GNNs):**\n",
    "    * Utilizan representaciones latentes de nodos y aprenden conexiones adaptativas entre ellos.\n",
    "2. **Variational Graph Autoencoders (VGAE):**\n",
    "    * Utilizan técnicas de autoencoders variacionales para aprender tanto representaciones de nodos como relaciones implícitas (aristas).\n",
    "3. **Graph Attention Networks (GAT):**\n",
    "    * Introducen atención para ponderar la importancia de las conexiones entre nodos.\n",
    "4. **Métodos Probabilísticos:**\n",
    "    * Modelos como Probabilistic Graphical Models (PGM) y enfoques bayesianos infieren relaciones con base en distribuciones de probabilidad.\n",
    "5. **Contrastive Learning:**\n",
    "    * Aprende representaciones de grafos al maximizar la similitud entre nodos conectados implícitamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89458a7c-a996-44ea-bd34-f076896aaddf",
   "metadata": {},
   "source": [
    "---\n",
    "### **1. Graph Neural Networks (GNNs)**\n",
    "Los **GNNs** son redes neuronales diseñadas específicamente para datos en forma de grafos. En el contexto de **Latent Graph Inference**, los GNNs pueden aprender no solo representaciones latentes de los nodos, sino también inferir relaciones adaptativas (es decir, las conexiones o pesos de las aristas).\n",
    "\n",
    "#### **Componentes principales:**\n",
    "1. **Propagación de mensajes:**\n",
    "    * Los nodos intercambian información con sus vecinos.\n",
    "    * Cada nodo $v$ actualiza su representación usando un agregador de las características de sus vecinos $(N(v))$ y sus propias características:\n",
    "$$\n",
    "h_i^{(k+1)} = \\text{AGGREGATE}(h_u^{(k)},\\forall u \\in N(v))\n",
    "$$\n",
    "    * Ejemplo de agregador: suma, promedio o función de atención.\n",
    "2. **Representaciones latentes:**\n",
    "    * Al final del entrenamiento, cada nodo tiene una representación $h_v$ en un espacio latente que captura sus relaciones y contexto local en el grafo.\n",
    "  \n",
    "#### **Ventaja en Latent Graph Inference:**\n",
    "Si el grafo original no está explícito, los GNN pueden comenzar con un grafo completamente conectado (conexiones densas) y aprender qué conexiones son importantes ajustando los pesos de las aristas.\n",
    "\n",
    "#### **Variantes de GNNs:**\n",
    "1. **Graph Convolutional Networks (GCNs):**\n",
    "    * Introducidas por Kipf y Welling (2017).\n",
    "    * Usan convoluciones espectrales para mezclar información de nodos conectados:\n",
    "$$\n",
    "H^{(k+1)} = \\sigma(\\hat{D}^{−1/2}\\hat{A}\\hat{D}^{−1/2}H^{(k)}W^{(k)})\n",
    "$$\n",
    "Donde $\\hat{A}$ es la matriz de adyacencia normalizada y $W$ son pesos aprendibles.\n",
    "2. **Graph Attention Networks (GATs):**\n",
    "    * Asignan diferentes pesos a las conexiones de un nodo utilizando un mecanismo de atención.\n",
    "    * Calculan coeficientes de atención $\\alpha_{ij}$ entre un nodo $i$ y sus vecinos $j$:\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(\\text{LeakyReLU}(a^T[Wh_i||Wh_j]))}{\\sum_{k\\in N(i)}\\exp(\\text{LeakyReLU}(a^T[Wh_i||Wh_k]))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ea2b24-10e3-4bf6-b896-795683d5a1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5992, -0.7969],\n",
       "        [-0.5992, -0.7969],\n",
       "        [-0.8469, -0.5599],\n",
       "        [-0.8469, -0.5599]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Datos de ejemplo: 4 nodos, 2 características por nodo\n",
    "x = torch.tensor([[1, 2], [2, 3], [3, 1], [4, 5]], dtype=torch.float)\n",
    "edge_index = torch.tensor([[0, 1, 2, 3], [1, 0, 3, 2]], dtype=torch.long)  # Aristas bidireccionales\n",
    "\n",
    "# Crear grafo\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Modelo GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 4)\n",
    "        self.conv2 = GCNConv(4, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN(in_channels=2, out_channels=2)\n",
    "output = model(data)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0a547d-3c00-4ba1-be6b-e6e9f9f8c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test Accuracy: 0.5350\n",
      "Epoch 10, Test Accuracy: 0.7990\n",
      "Epoch 20, Test Accuracy: 0.7990\n",
      "Epoch 30, Test Accuracy: 0.7950\n",
      "Epoch 40, Test Accuracy: 0.8000\n",
      "Epoch 50, Test Accuracy: 0.7990\n",
      "Epoch 60, Test Accuracy: 0.8050\n",
      "Epoch 70, Test Accuracy: 0.8080\n",
      "Epoch 80, Test Accuracy: 0.8120\n",
      "Epoch 90, Test Accuracy: 0.8120\n",
      "Epoch 100, Test Accuracy: 0.8130\n",
      "Epoch 110, Test Accuracy: 0.8120\n",
      "Epoch 120, Test Accuracy: 0.8120\n",
      "Epoch 130, Test Accuracy: 0.8070\n",
      "Epoch 140, Test Accuracy: 0.8060\n",
      "Epoch 150, Test Accuracy: 0.8040\n",
      "Epoch 160, Test Accuracy: 0.8030\n",
      "Epoch 170, Test Accuracy: 0.8020\n",
      "Epoch 180, Test Accuracy: 0.8050\n",
      "Epoch 190, Test Accuracy: 0.8060\n"
     ]
    }
   ],
   "source": [
    "#pip install torch-geometric\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Cargar el dataset Cora\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Modelo GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = GCN(in_channels=dataset.num_features, out_channels=dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = correct / data.test_mask.sum()\n",
    "    return acc\n",
    "\n",
    "for epoch in range(200):\n",
    "    train()\n",
    "    acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cbb46-6735-4112-b57a-33afe076f9c6",
   "metadata": {},
   "source": [
    "---\n",
    "### **2. Variational Graph Autoencoders (VGAEs)**\n",
    "Los **VGAEs** son un enfoque específico para inferir grafos. Utilizan técnicas de autoencoders variacionales para aprender tanto representaciones de nodos como la matriz de adyacencia implícita.\n",
    "\n",
    "#### **Arquitectura:**\n",
    "1. Encoder:\n",
    "    * Una red neuronal (como un GCN) mapea las características de los nodos a un espacio latente:\n",
    "$$\n",
    "Z = \\text{GCN}(X,A)\n",
    "$$\n",
    "Donde $Z$ son las representaciones latentes, $X$ las características iniciales y $A$ la matriz de adyacencia (que puede ser inicial o inferida).\n",
    "2. Decoder:\n",
    "    * Reconstruye la matriz de adyacencia $\\hat{A}$ usando un producto interno en el espacio latente:\n",
    "$$\n",
    "\\hat{A}_{ij} = \\sigma(Z_i^TZ_j)\n",
    "$$\n",
    "3. Pérdida:\n",
    "    * Combina una pérdida de reconstrucción (para $A$) y una pérdida de regularización (KL-divergence):\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{q(Z|X,A)}[\\log p(A|Z)] - \\text{KL}(q(Z|X,A)||p(Z))\n",
    "$$\n",
    "\n",
    "#### **Ventaja en Latent Graph Inference:**\n",
    "Pueden inferir conexiones incluso con datos incompletos, proporcionando una matriz de adyacencia plausible al decodificar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea3f228-a76f-4fc7-a9d0-cad6d3f99ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# VGAE Model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m VGAE(Encoder(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_all(z)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatent Representations (z):\u001b[39m\u001b[38;5;124m\"\u001b[39m, z)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch_geometric/nn/models/autoencoder.py:169\u001b[0m, in \u001b[0;36mVGAE.encode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mMAX_LOGSTD)\n\u001b[1;32m    171\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "\n",
    "# Encoder para VGAE\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Datos de ejemplo\n",
    "x = torch.tensor([[1, 2], [2, 3], [3, 1], [4, 5]], dtype=torch.float)\n",
    "edge_index = torch.tensor([[0, 1, 2, 3], [1, 0, 3, 2]], dtype=torch.long)  # Aristas bidireccionales\n",
    "\n",
    "# VGAE Model\n",
    "model = VGAE(Encoder(in_channels=2, out_channels=2))\n",
    "z = model.encode(x, edge_index)\n",
    "reconstructed = model.decode_all(z)\n",
    "print(\"Latent Representations (z):\", z)\n",
    "print(\"Reconstructed Adjacency Matrix:\", reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b2fd0f-9e74-4fa9-a618-338f2f1d8b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 10\n",
      "Epoch 20\n",
      "Epoch 30\n",
      "Epoch 40\n",
      "Epoch 50\n",
      "Epoch 60\n",
      "Epoch 70\n",
      "Epoch 80\n",
      "Epoch 90\n",
      "Epoch 100\n",
      "Epoch 110\n",
      "Epoch 120\n",
      "Epoch 130\n",
      "Epoch 140\n",
      "Epoch 150\n",
      "Epoch 160\n",
      "Epoch 170\n",
      "Epoch 180\n",
      "Epoch 190\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import VGAE, GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Cargar dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# Definir Encoder para VGAE\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        self.conv_mu = GCNConv(16, out_channels)\n",
    "        self.conv_logvar = GCNConv(16, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "# Inicializar VGAE\n",
    "model = VGAE(Encoder(dataset.num_features, 2))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    loss = model.recon_loss(z, data.edge_index)\n",
    "    kl_loss = model.kl_loss()\n",
    "    loss = loss + 0.01 * kl_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(200):\n",
    "    train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21c44c-7418-435f-a97b-7250fc5ee1e4",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Graph Attention Networks (GATs)**\n",
    "Los **GATs** son una mejora de los GCNs que introducen atención para aprender la importancia relativa de las conexiones entre nodos.\n",
    "\n",
    "#### **Características principales:**\n",
    "1. Mecanismo de atención:\n",
    "    * Cada nodo evalúa la importancia de sus vecinos usando coeficientes de atención $\\alpha_{ij}$, que se calculan para cada arista:\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}), e_{ij} = \\text{LeakyReLU}(a^Tf[Wh_i||Wh_j])\n",
    "$$\n",
    "Donde $a$ es un vector de pesos de atención aprendible.\n",
    "2. Actualización de nodos:\n",
    "    * La representación de cada nodo se actualiza como una combinación ponderada de las características de sus vecinos:\n",
    "$$\n",
    "h_i^{(k+1)} = \\sigma(\\sum_{i\\in N(i)} \\alpha_{ij}Wh_j)\n",
    "$$\n",
    "#### **Ventaja en Latent Graph Inference:**\n",
    "* Permiten inferir grafos más precisos al enfocarse en conexiones relevantes y descartar las irrelevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e5e83f-2af6-485d-9c5d-695b8bf67a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1587, -1.9189],\n",
      "        [-0.1587, -1.9189],\n",
      "        [-0.0534, -2.9574],\n",
      "        [-0.0534, -2.9574]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Datos de ejemplo\n",
    "x = torch.tensor([[1, 2], [2, 3], [3, 1], [4, 5]], dtype=torch.float)\n",
    "edge_index = torch.tensor([[0, 1, 2, 3], [1, 0, 3, 2]], dtype=torch.long)\n",
    "\n",
    "# Modelo GAT\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, 8, heads=4, concat=True)\n",
    "        self.conv2 = GATConv(8 * 4, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        return F.log_softmax(self.conv2(x, edge_index), dim=1)\n",
    "\n",
    "model = GAT(in_channels=2, out_channels=2)\n",
    "output = model(x, edge_index)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeba543c-6262-4de6-b05c-3c3a2e7760ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test Accuracy: 0.5870\n",
      "Epoch 10, Test Accuracy: 0.6660\n",
      "Epoch 20, Test Accuracy: 0.6760\n",
      "Epoch 30, Test Accuracy: 0.6760\n",
      "Epoch 40, Test Accuracy: 0.6820\n",
      "Epoch 50, Test Accuracy: 0.6920\n",
      "Epoch 60, Test Accuracy: 0.6920\n",
      "Epoch 70, Test Accuracy: 0.6960\n",
      "Epoch 80, Test Accuracy: 0.6920\n",
      "Epoch 90, Test Accuracy: 0.6910\n",
      "Epoch 100, Test Accuracy: 0.6850\n",
      "Epoch 110, Test Accuracy: 0.6840\n",
      "Epoch 120, Test Accuracy: 0.6830\n",
      "Epoch 130, Test Accuracy: 0.6820\n",
      "Epoch 140, Test Accuracy: 0.6810\n",
      "Epoch 150, Test Accuracy: 0.6840\n",
      "Epoch 160, Test Accuracy: 0.6840\n",
      "Epoch 170, Test Accuracy: 0.6830\n",
      "Epoch 180, Test Accuracy: 0.6830\n",
      "Epoch 190, Test Accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Cargar dataset Citeseer\n",
    "dataset = Planetoid(root='/tmp/Citeseer', name='Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Definir modelo GAT\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, 8, heads=8, concat=True)\n",
    "        self.conv2 = GATConv(8 * 8, out_channels, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        return F.log_softmax(self.conv2(x, edge_index), dim=1)\n",
    "\n",
    "# Modelo y optimizador\n",
    "model = GAT(in_channels=dataset.num_features, out_channels=dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Entrenamiento\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = correct / data.test_mask.sum()\n",
    "    return acc\n",
    "\n",
    "for epoch in range(200):\n",
    "    train()\n",
    "    acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d69e2-d1e6-4578-a621-8006d8a6747e",
   "metadata": {},
   "source": [
    "---\n",
    "### **4. Métodos Probabilísticos**\n",
    "\n",
    "Estos métodos infieren grafos latentes desde un enfoque estadístico, usando distribuciones de probabilidad para modelar las relaciones.\n",
    "\n",
    "#### **Ejemplo: Probabilistic Graphical Models (PGMs):**\n",
    "1. **Modelo básico:**\n",
    "    * Representan relaciones condicionales entre nodos usando nodos y aristas.\n",
    "    * Ejemplo: Redes bayesianas o modelos de Markov.\n",
    "2. **Inferencia:**\n",
    "    * Utilizan técnicas como inferencia variacional o Gibbs Sampling para inferir conexiones probables entre nodos.\n",
    "\n",
    "#### **Ventaja en Latent Graph Inference:**\n",
    "Son ideales cuando se dispone de información previa probabilística sobre los nodos o sus posibles relaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c9ac2b-7cd3-4df7-a858-bd4a8452b21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pgmpy\n",
      "  Downloading pgmpy-0.1.26-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (3.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (1.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (2.2.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (3.1.2)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (2.4.1)\n",
      "Requirement already satisfied: statsmodels in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (0.14.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (4.66.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (1.4.2)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (3.3.0)\n",
      "Requirement already satisfied: xgboost in /opt/conda/envs/umap/lib/python3.9/site-packages (from pgmpy) (2.1.1)\n",
      "Collecting google-generativeai (from pgmpy)\n",
      "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai->pgmpy)\n",
      "  Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai->pgmpy)\n",
      "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai->pgmpy)\n",
      "  Downloading google_api_python_client-2.154.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai->pgmpy)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/envs/umap/lib/python3.9/site-packages (from google-generativeai->pgmpy) (4.25.3)\n",
      "Collecting pydantic (from google-generativeai->pgmpy)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m699.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/umap/lib/python3.9/site-packages (from google-generativeai->pgmpy) (4.12.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/umap/lib/python3.9/site-packages (from pandas->pgmpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/umap/lib/python3.9/site-packages (from pandas->pgmpy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/umap/lib/python3.9/site-packages (from pandas->pgmpy) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from scikit-learn->pgmpy) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/envs/umap/lib/python3.9/site-packages (from statsmodels->pgmpy) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/envs/umap/lib/python3.9/site-packages (from statsmodels->pgmpy) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (3.15.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch->pgmpy) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/umap/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->pgmpy) (12.6.20)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai->pgmpy)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from google-api-core->google-generativeai->pgmpy) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai->pgmpy)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai->pgmpy)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai->pgmpy)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six in /opt/conda/envs/umap/lib/python3.9/site-packages (from patsy>=0.5.6->statsmodels->pgmpy) (1.16.0)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai->pgmpy)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai->pgmpy)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai->pgmpy)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from jinja2->torch->pgmpy) (2.1.3)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai->pgmpy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic->google-generativeai->pgmpy)\n",
      "  Downloading pydantic_core-2.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions (from google-generativeai->pgmpy)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from sympy->torch->pgmpy) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/umap/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy) (1.64.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pgmpy)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/umap/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/umap/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/umap/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/umap/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2024.6.2)\n",
      "Collecting protobuf (from google-generativeai->pgmpy)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai->pgmpy)\n",
      "  Downloading grpcio-1.68.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Downloading pgmpy-0.1.26-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_python_client-2.154.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.7/221.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, typing-extensions, pyasn1, protobuf, httplib2, grpcio, cachetools, annotated-types, rsa, pydantic-core, pyasn1-modules, proto-plus, googleapis-common-protos, pydantic, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, pgmpy\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jupyter-user7/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 cachetools-5.5.0 google-ai-generativelanguage-0.6.10 google-api-core-2.23.0 google-api-python-client-2.154.0 google-auth-2.36.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.3 googleapis-common-protos-1.66.0 grpcio-1.68.0 grpcio-status-1.68.0 httplib2-0.22.0 pgmpy-0.1.26 proto-plus-1.25.0 protobuf-5.28.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.2 pydantic-core-2.27.1 rsa-4.9 typing-extensions-4.12.2 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3020db-e140-4464-84cc-09a3efaf2dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "/opt/conda/envs/umap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
      "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| C    |   phi(C) |\n",
      "+======+==========+\n",
      "| C(0) |   0.5800 |\n",
      "+------+----------+\n",
      "| C(1) |   0.4200 |\n",
      "+------+----------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Crear un modelo gráfico bayesiano\n",
    "model = BayesianModel([('A', 'B'), ('B', 'C')])\n",
    "\n",
    "# Definir distribuciones de probabilidad\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "cpd_a = TabularCPD(variable='A', variable_card=2, values=[[0.6], [0.4]])\n",
    "cpd_b = TabularCPD(variable='B', variable_card=2,\n",
    "                   values=[[0.7, 0.2], [0.3, 0.8]],\n",
    "                   evidence=['A'], evidence_card=[2])\n",
    "cpd_c = TabularCPD(variable='C', variable_card=2,\n",
    "                   values=[[0.9, 0.5], [0.1, 0.5]],\n",
    "                   evidence=['B'], evidence_card=[2])\n",
    "\n",
    "# Asociar CPDs al modelo\n",
    "model.add_cpds(cpd_a, cpd_b, cpd_c)\n",
    "\n",
    "# Inferencia\n",
    "infer = VariableElimination(model)\n",
    "print(infer.query(['C'], evidence={'A': 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f4eb65-e67c-4398-87c5-c0ec0e37ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| C    |   phi(C) |\n",
      "+======+==========+\n",
      "| C(0) |   0.5800 |\n",
      "+------+----------+\n",
      "| C(1) |   0.4200 |\n",
      "+------+----------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Crear modelo\n",
    "model = BayesianNetwork([('A', 'B'), ('B', 'C')])\n",
    "\n",
    "# Definir CPDs\n",
    "cpd_a = TabularCPD(variable='A', variable_card=2, values=[[0.6], [0.4]])\n",
    "cpd_b = TabularCPD(variable='B', variable_card=2, values=[[0.7, 0.2], [0.3, 0.8]], evidence=['A'], evidence_card=[2])\n",
    "cpd_c = TabularCPD(variable='C', variable_card=2, values=[[0.9, 0.5], [0.1, 0.5]], evidence=['B'], evidence_card=[2])\n",
    "\n",
    "model.add_cpds(cpd_a, cpd_b, cpd_c)\n",
    "\n",
    "# Inferencia\n",
    "infer = VariableElimination(model)\n",
    "print(infer.query(['C'], evidence={'A': 1}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b745b6-e388-4d64-9c09-90ac3c6c1242",
   "metadata": {},
   "source": [
    "---\n",
    "### **5. Contrastive Learning**\n",
    "Este enfoque aprende grafos latentes maximizando la similitud entre nodos conectados implícitamente y minimizando la similitud entre nodos no conectados.\n",
    "\n",
    "#### **Ejemplo: Graph Contrastive Learning (GCL):**\n",
    "1. Objetivo:\n",
    "    * Maximizar la similitud entre nodos $u, v$ si $u∼v$ y minimizarla si no están conectados:\n",
    "$$\n",
    "\\mathcal{L}_{contrastive} = -\\log \\frac{\\exp(\\text{sim}(h_u, h_v))}{\\sum_k \\exp(\\text{sim}(h_u, h_k))}\n",
    "$$\n",
    "\n",
    "2. Similitud:\n",
    "    * Se mide con funciones como el producto punto o la distancia euclidiana.\n",
    "\n",
    "#### **Ventaja en Latent Graph Inference:**\n",
    "Aprende representaciones robustas que reflejan relaciones implícitas, incluso con ruido en los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81680a0e-a0e3-4cd5-b175-0dd3e2105a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/envs/umap/lib/python3.9/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jupyter-user7/.local/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/umap/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/umap/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6fa8ba-f356-440a-ac26-c7ef0ef6c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: tensor(0.5304)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "# Representaciones de ejemplo\n",
    "h1 = torch.tensor([[1.0, 0.5], [0.5, 0.3]], dtype=torch.float)\n",
    "h2 = torch.tensor([[1.1, 0.4], [0.6, 0.2]], dtype=torch.float)\n",
    "\n",
    "# Similitudes positivas (entre nodos conectados)\n",
    "similarity = CosineSimilarity(dim=1)(h1, h2)\n",
    "\n",
    "# Pérdida contrastiva\n",
    "negative_samples = torch.tensor([[0.1, 0.9], [0.2, 0.8]], dtype=torch.float)\n",
    "negative_similarity = CosineSimilarity(dim=1)(h1, negative_samples)\n",
    "\n",
    "loss = -torch.log(torch.exp(similarity) / (torch.exp(similarity) + torch.exp(negative_similarity)))\n",
    "print(\"Contrastive Loss:\", loss.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec5656e-aee5-4747-9a3f-6e6429e9718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: tensor(0.5304)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "# Representaciones de ejemplo\n",
    "h1 = torch.tensor([[1.0, 0.5], [0.5, 0.3]], dtype=torch.float)\n",
    "h2 = torch.tensor([[1.1, 0.4], [0.6, 0.2]], dtype=torch.float)\n",
    "\n",
    "# Similitudes positivas (entre nodos conectados)\n",
    "similarity = CosineSimilarity(dim=1)(h1, h2)\n",
    "\n",
    "# Pérdida contrastiva\n",
    "negative_samples = torch.tensor([[0.1, 0.9], [0.2, 0.8]], dtype=torch.float)\n",
    "negative_similarity = CosineSimilarity(dim=1)(h1, negative_samples)\n",
    "\n",
    "loss = -torch.log(torch.exp(similarity) / (torch.exp(similarity) + torch.exp(negative_similarity)))\n",
    "print(\"Contrastive Loss:\", loss.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a76ba-3dc7-4e56-930d-6f71017ef053",
   "metadata": {},
   "source": [
    "## **Resumen Comparativo**\n",
    "| Método | Ventajas | Desventajas |\n",
    "|---|---|---|\n",
    "| GNNs | Escalables, versátiles | Menos específicas para grafos latentes |\n",
    "| VGAEs | Dedican atención explícita a inferir grafos | Costo computacional alto |\n",
    "| GATs | Ponderan relaciones adaptativamente | Complejidad adicional por atención |\n",
    "| Métodos Probabilísticos | Basados en principios estadísticos sólidos | Requieren suposiciones probabilísticas |\n",
    "| Contrastive Learning | Robusto contra ruido | Sensible a la elección de datos negativos |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6a846-accc-4541-bdd1-c684f401abaa",
   "metadata": {},
   "source": [
    "### **Pasos en Latent Graph Inference**\n",
    "1. **Preparación de los Datos:**\n",
    "    * Los datos pueden ser características de nodos sin un grafo explícito.\n",
    "2. **Modelo:**\n",
    "    * Diseñar un modelo capaz de aprender una estructura latente de grafo (por ejemplo, GNN o VGAE).\n",
    "3. **Inferencia:**\n",
    "    * Durante el entrenamiento, se optimiza una representación latente de los nodos y una matriz de adyacencia que capture las relaciones implícitas.\n",
    "4. **Evaluación:**\n",
    "    * La calidad del grafo inferido puede evaluarse comparándolo con datos reales (si existen) o evaluando el rendimiento en una tarea específica (como clasificación de nodos).\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcab71-2434-4b66-b29e-ca1a36ea4964",
   "metadata": {},
   "source": [
    "### **Aplicaciones**\n",
    "* **Biología:** Inferir redes genéticas, interacciones proteína-proteína.\n",
    "* **Redes Sociales:** Detectar comunidades ocultas o relaciones implícitas entre usuarios.\n",
    "* **Procesamiento de Lenguaje Natural:** Extraer dependencias semánticas entre palabras o conceptos.\n",
    "* **Sistemas de Recomendación:** Inferir conexiones entre usuarios y productos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d69b8-3b8e-47d2-8d39-4387217db2be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "755b01da-be63-41ea-8803-236252fb8b50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c036609-5b43-47a0-9c5c-0cf8bb540b76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22ccab43-8e29-4516-a30b-cee450010694",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umap",
   "language": "python",
   "name": "umap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
